{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DALLAS CRIME DATA</center></h1>\n",
    "<h2><center>MSDS 7331 Mini Project</center></h2>\n",
    "<h2><center>SVM and Logistic Regression Modeling</center></h2>\n",
    "\n",
    "<h4><center>Team Members</center></h4>\n",
    "\n",
    "           Yejur Singh Kunwar           Bin Yu               Vivek Viswanathan          Kevin Mendonsa\n",
    "              Dallas, TX               Dallas, TX               Dallas, TX                Irvine, CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We will be using Dallas crime datset where police incidents were recorded. For this miniproject, main dataframe that we will be utilizing is the saved dataframe from LAB 1, details for this dataset can be found in LAB 1.  \n",
    "\n",
    "The data shape was 255154 observations with 44 attributes.\n",
    "\n",
    "- Logistic Regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Logistic Regression Model vs. Support Vector Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be deleted\n",
    "\n",
    "[50 points] Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing necessary packages for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages for python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "from dateutil.relativedelta import relativedelta\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "# plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "from itertools import cycle, islice\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import metrics as mt\n",
    "from __future__ import print_function, division\n",
    "from patsy import dmatrices, dmatrix\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data/\" # Generic path\n",
    "incident = pd.read_csv(path + 'LAB1_completed_Dataset_clean.csv', low_memory= False)\n",
    "#-------------------------------\n",
    "# LOADING SUPPORTING DATA SETS\n",
    "#-------------------------------\n",
    "# Dallas area population dataset\n",
    "#-------------------------------\n",
    "population = pd.read_csv(path + 'Population.csv', low_memory=False)\n",
    "#--------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    30288\n",
       "Name: Response_time, dtype: int64"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30288 rows has 0 respone time which is incident is cancealled or didn't dispatch\n",
    "incident[incident['Response_time']==0]['Response_time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 0 response time records from the data set.\n",
    "incident = incident[incident['Response_time']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incident.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\numpy\\lib\\histograms.py:746: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  keep = (tmp_a >= first_edge)\n",
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\numpy\\lib\\histograms.py:747: RuntimeWarning: invalid value encountered in less_equal\n",
      "  keep &= (tmp_a <= last_edge)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([101980.,  38130.,  22772.,  14385.,  12274.,   9720.,   7214.,\n",
       "          6939.,   5653.,   4715.]),\n",
       " array([ 0. ,  9.7, 19.4, 29.1, 38.8, 48.5, 58.2, 67.9, 77.6, 87.3, 97. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFJCAYAAACCQLQfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw histogram for response time attibute\n",
    "plt.hist(incident['Response_time'],range=[0,97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the median of the response time \n",
    "from numpy import median\n",
    "median(incident['Response_time'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    223782.000000\n",
       "mean         21.871299\n",
       "std          23.859860\n",
       "min           1.000000\n",
       "25%           4.000000\n",
       "50%          11.000000\n",
       "75%          32.000000\n",
       "max          97.000000\n",
       "Name: Response_time, dtype: float64"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incident['Response_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident.loc[incident['Weapon_Used'] == 'None', 'Weapon_Used_Fact'] = 0\n",
    "incident.loc[incident['Weapon_Used'] != 'None', 'Weapon_Used_Fact'] = 1\n",
    "if 'Weapon_Used' in incident:\n",
    "    del incident['Weapon_Used'] # get rid of the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCR_Offense_Name change this\n",
    "\n",
    "incident.loc[:,'UCR_Offense_Name'] = incident['UCR_Offense_Name'].fillna(\"MISSING\")\n",
    "\n",
    "THEFT_FRAUD     = dict.fromkeys(['THEFT/BMV', 'THEFT ORG RETAIL', 'BURGLARY-RESIDENCE', 'OTHER THEFTS',\n",
    "                                 'ROBBERY-INDIVIDUAL','THEFT/SHOPLIFT', 'BURGLARY-BUSINESS', 'FORGE & COUNTERFEIT', \n",
    "                                 'FRAUD', 'EMBEZZLEMENT','ROBBERY-BUSINESS','THEFT ORG RETAIL'],\"THEFT_FRAUD\" ) \n",
    "MVA_TRAFFIC      =dict.fromkeys(['ACCIDENT MV', 'MOTOR VEHICLE ACCIDENT', 'UUMV', 'TRAFFIC VIOLATION',\n",
    "                                 'TRAFFIC FATALITY'],\"MVA_TRAFFIC\" )        \n",
    "WEAPONS_FIREARMS =dict.fromkeys(['WEAPONS', 'ARSON', 'INJURED FIREARM'], \"WEAPONS_FIREARMS\")         \n",
    "ASSUALT          = dict.fromkeys(['ASSAULT','VANDALISM & CRIM MISCHIEF', 'AGG ASSAULT - NFV', 'OFFENSE AGAINST CHILD',\n",
    "                                  'AGG ASSAULT - FV'], \"ASSUALT\")\n",
    "OTHERS_THREATS   = dict.fromkeys(['FOUND', 'OTHERS', 'LOST', 'CRIMINAL TRESPASS', 'DISORDERLY CONDUCT', \n",
    "                                  'ANIMAL BITE','INJURED HOME','INJURED PUBLIC', 'TERRORISTIC THREAT', \n",
    "                                  'EVADING', 'INJURED OCCUPA', 'ORANIZED CRIME', 'KIDNAPPING', \n",
    "                                  'RESIST ARREST','FAIL TO ID', 'HUMAN TRAFFICKING', 'MISSING'], \"OTHERS_THREATS\")\n",
    "INTOXICATION     = dict.fromkeys(['DRUNK & DISORDERLY', 'DWI', 'NARCOTICS & DRUGS', 'LIQUOR OFFENSE', \n",
    "                                  'INTOXICATION MANSLAUGHTER'],\"INTOXICATION\")\n",
    "MURDER_DEATH     = dict.fromkeys(['SUDDEN DEATH&FOUND BODIES','MURDER'], \"MURDER_DEATH\")\n",
    "                    \n",
    "\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(THEFT_FRAUD)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(MVA_TRAFFIC)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(WEAPONS_FIREARMS)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(ASSUALT)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(OTHERS_THREATS)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(INTOXICATION)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(MURDER_DEATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FILTERING OUT UNNECESSARY NULL DATA\n",
    "incident = incident[incident['Watch']!=0]\n",
    "incident = incident[(incident['Victim_Age']>=0) & (incident['Victim_Age']<=90)]\n",
    "incident = incident[incident['Victim_Race']!=\"Unknown\"]\n",
    "incident = incident[incident['Victim_Type']!=\"Unknown\"]\n",
    "incident= incident[incident.Number_of_offense != \"RP\"]\n",
    "incident.loc[:,'Social_crime_score'] = incident['Hate_Crime']+incident['Gang_Related_Offense']+incident['Drug_Related']+incident['Weapon_Used_Fact']\n",
    "incident = incident[incident['Victim_Gender']!=\"U\"]\n",
    "incident.loc[:,'IsMale'] = incident.Victim_Gender=='M' \n",
    "incident.IsMale = incident.IsMale.astype(np.int)\n",
    "incident.loc[:,'Number_of_offense']= incident.Number_of_offense.astype(np.int)\n",
    "\n",
    "incident.loc[:,'Victim_Age'] = incident['Victim_Age'].astype(np.int)\n",
    "\n",
    "incident.loc[:,'Response_time'] = incident['Response_time'].fillna(incident['Response_time'].mean()).astype(np.int)\n",
    "# 0 - fast, 1 - middium, 2 - low\n",
    "incident.loc[:,'Responsetime_cat'] = pd.cut(incident.Response_time,[0,11,25,999],3,labels=[0,1,2])\n",
    "\n",
    "# incident= incident[incident['Responsetime_cat']!=np.nan]\n",
    "\n",
    "incident['UCR_Offense_Name'] = pd.Categorical(incident['UCR_Offense_Name']).codes\n",
    "incident['Day1_of_the_Week'] = pd.Categorical(incident['Day1_of_the_Week']).codes\n",
    "incident['Division'] = pd.Categorical(incident['Division']).codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "incident['Degree']=incident['RMS_Code'].astype(str).str[:2]\n",
    "incident['Degree_Fact'] = pd.Categorical(incident['Degree']).codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 201689 entries, 0 to 255153\n",
      "Data columns (total 44 columns):\n",
      "Year_of_Incident           201689 non-null int64\n",
      "Watch                      201689 non-null int64\n",
      "Type_of_Incident           201689 non-null object\n",
      "Type_Location              198121 non-null object\n",
      "Reporting_Area             201566 non-null float64\n",
      "Beat                       201637 non-null float64\n",
      "Division                   201689 non-null int8\n",
      "Sector                     201637 non-null float64\n",
      "Council_District           201689 non-null object\n",
      "Day1_of_the_Week           201689 non-null int8\n",
      "Person_Involvement_Type    201689 non-null object\n",
      "Victim_Type                201689 non-null object\n",
      "Victim_Race                201689 non-null object\n",
      "Victim_Gender              201689 non-null object\n",
      "Victim_Age                 201689 non-null int32\n",
      "Offense_Status             201324 non-null object\n",
      "Victim_Condition           17122 non-null object\n",
      "Hate_Crime                 201689 non-null int64\n",
      "Gang_Related_Offense       201689 non-null int64\n",
      "Drug_Related               201689 non-null int64\n",
      "UCR_Offense_Name           201689 non-null int8\n",
      "RMS_Code                   201689 non-null object\n",
      "UCR_Code                   190624 non-null float64\n",
      "X_Coordinate               185531 non-null float64\n",
      "Y_Coordinate               185531 non-null float64\n",
      "Zip_Code                   201689 non-null int64\n",
      "City                       201689 non-null object\n",
      "State                      201205 non-null object\n",
      "Location1                  201689 non-null object\n",
      "Call_Received              200780 non-null object\n",
      "Call_Cleared               200646 non-null object\n",
      "Call_Dispatch              200762 non-null object\n",
      "Number_of_offense          201689 non-null int32\n",
      "Response_time              201689 non-null int32\n",
      "Latitude                   193740 non-null float64\n",
      "Longitude                  193740 non-null float64\n",
      "Arrest_status              201689 non-null int64\n",
      "Call_Received_Hour         200780 non-null float64\n",
      "Weapon_Used_Fact           201689 non-null float64\n",
      "Social_crime_score         201689 non-null float64\n",
      "IsMale                     201689 non-null int32\n",
      "Responsetime_cat           201689 non-null category\n",
      "Degree                     201689 non-null object\n",
      "Degree_Fact                201689 non-null int8\n",
      "dtypes: category(1), float64(11), int32(4), int64(7), int8(4), object(17)\n",
      "memory usage: 59.4+ MB\n"
     ]
    }
   ],
   "source": [
    "incident.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division            Y_Coordinate         -0.643603\n",
      "Sector              X_Coordinate         -0.431452\n",
      "Beat                X_Coordinate         -0.429325\n",
      "Reporting_Area      X_Coordinate         -0.352009\n",
      "UCR_Offense_Name    Degree_Fact          -0.278610\n",
      "Year_of_Incident    Longitude            -0.212045\n",
      "                    Latitude             -0.212019\n",
      "Arrest_status       UCR_Offense_Name     -0.198657\n",
      "Y_Coordinate        Reporting_Area       -0.141704\n",
      "Division            X_Coordinate         -0.133813\n",
      "                                            ...   \n",
      "                    Beat                  0.296908\n",
      "Beat                Reporting_Area        0.332316\n",
      "Sector              Reporting_Area        0.332536\n",
      "Year_of_Incident    Social_crime_score    0.503496\n",
      "UCR_Code            Degree_Fact           0.507150\n",
      "Weapon_Used_Fact    Year_of_Incident      0.540173\n",
      "Call_Received_Hour  Watch                 0.679547\n",
      "Weapon_Used_Fact    Social_crime_score    0.940273\n",
      "Longitude           Latitude              0.999894\n",
      "Sector              Beat                  0.999942\n",
      "Length: 325, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Create correlation matrix\n",
    "CorrMatrix = incident.corr()\n",
    "\n",
    "# Highest Correlation Pairs\n",
    "corrPairs = CorrMatrix.unstack().sort_values(kind=\"quicksort\")\n",
    "#- REMOVE DUPLICATES\n",
    "corrPairs = corrPairs[::2]\n",
    "corrPairs = corrPairs[corrPairs.index.get_level_values(0) != corrPairs.index.get_level_values(1)]\n",
    "with pd.option_context('display.max_rows',20):\n",
    "    print(corrPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS\n",
    "\n",
    "#### Preparation of Data\n",
    "\n",
    "The incident dataset is further cleaned and and prepared in right format to feed into model algorithm. \n",
    "\n",
    "The attributes where null or empty values were replaced with Unknows or other category is removed in order to reduce data ambuigity in the model for selected attributes. \n",
    "\n",
    "The data is splitted to response variable (response_category_classification) which will be called \"Y\" and explanatory variables or attributes \"X\" for placing it into the algoirith model. \n",
    "\n",
    "The main dataset is splitted into training and test dataset by 80/20 split method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "inci_imp = incident[['Day1_of_the_Week',  'Division',\n",
    "#                      'Response_time',\n",
    "                     'Responsetime_cat','Arrest_status','Social_crime_score', 'IsMale','Call_Received_Hour',\n",
    "                    'UCR_Offense_Name',\n",
    "                    ]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Day1_of_the_Week      0\n",
       "Division              0\n",
       "Responsetime_cat      0\n",
       "Arrest_status         0\n",
       "Social_crime_score    0\n",
       "IsMale                0\n",
       "Call_Received_Hour    0\n",
       "UCR_Offense_Name      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inci_imp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING AND TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Day1_of_the_Week', 'Division', 'Responsetime_cat', 'Arrest_status',\n",
       "       'Social_crime_score', 'IsMale', 'Call_Received_Hour',\n",
       "       'UCR_Offense_Name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inci_imp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "#if 'UCR_Offense_Name' in inci_imp:\n",
    "y = inci_imp['Responsetime_cat'].values # get the labels we want\n",
    "del inci_imp['Responsetime_cat'] # get rid of the class label\n",
    "X = inci_imp.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "# features = ['Day1_of_the_Week',  'Division',\n",
    "# #                      'Response_time',\n",
    "#                      'Responsetime_cat','Arrest_status','Social_crime_score', 'IsMale','Call_Received_Hour',\n",
    "#                      'UCR_Offense_Name'\n",
    "#                     ]\n",
    "# # Create a new matplotlib figure\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot()\n",
    "# labels = list(map(lambda s: s.title(), features))\n",
    "# viz = FeatureImportances(GradientBoostingClassifier(),labels=labels, ax=ax)\n",
    "# viz.fit(X, y)\n",
    "# viz.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Logistic Regression\n",
    "Now let's use Logistic Regression from `scikit-learn`. The documentation can be found here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.5098615399940233\n",
      "confusion matrix\n",
      " [[19949     0   592]\n",
      " [ 7435     0   260]\n",
      " [11395     0   525]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 1  ====\n",
      "accuracy 0.508740910449248\n",
      "confusion matrix\n",
      " [[19944     0   547]\n",
      " [ 7438     0   273]\n",
      " [11469     0   485]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 2  ====\n",
      "accuracy 0.5082926586313378\n",
      "confusion matrix\n",
      " [[19889     0   567]\n",
      " [ 7435     0   313]\n",
      " [11430     0   522]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFXCAYAAACLEMbVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from yellowbrick.classifier import ClassBalance\n",
    "\n",
    "\n",
    "# Specify the features of interest and the classes of the target\n",
    "features = ['Day1_of_the_Week',  'Division',\n",
    "#                      'Response_time',\n",
    "                     'Arrest_status','Social_crime_score', 'IsMale','Call_Received_Hour',\n",
    "                    'UCR_Offense_Name',\n",
    "                    ]\n",
    "classes = [\"Fast\", \"Medium\",\"Low\"]\n",
    "\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "forest = RandomForestClassifier()\n",
    "visualizer = ClassBalance(forest, classes=classes)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.744645881063851\n",
      "confusion matrix\n",
      " [[29902     0]\n",
      " [10254     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 1  ====\n",
      "accuracy 0.7439236975794402\n",
      "confusion matrix\n",
      " [[29873     0]\n",
      " [10283     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 2  ====\n",
      "accuracy 0.7429524853073015\n",
      "confusion matrix\n",
      " [[29834     0]\n",
      " [10322     0]]\n"
     ]
    }
   ],
   "source": [
    "# this does the exact same thing as the above block of code, but with shorter syntax\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "take_nd() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-da4e413f2cc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# and here is an even shorter way of getting the accuracies for each training and test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_object\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# this also can help with parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    981\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    823\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[1;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0my_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0my_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\BINENV\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    183\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[0;32m    184\u001b[0m             \u001b[1;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: take_nd() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "# and here is an even shorter way of getting the accuracies for each training and test set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None) # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = inci_imp.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,inci_imp.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=inci_imp.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more improvement and guarding against overfitting: At this point it would make sense to remove variables that are highly related to one another or ones that are irrelevant and keep going with the weights analysis. What variables would you remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': inci_imp.columns,\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load the data\n",
    "# df = load_data('concrete')\n",
    "# feature_names = ['cement', 'slag', 'ash', 'water', 'splast', 'coarse', 'fine', 'age']\n",
    "# target_name = 'strength'\n",
    "\n",
    "# Get the X and y data from the DataFrame\n",
    "\n",
    "if 'Responsetime_cat' in inci_imp:\n",
    "    y = inci_imp['Responsetime_cat'].as_matrix() # get the labels we want\n",
    "    del inci_imp['Responsetime_cat'] # get rid of the class label\n",
    "    X = inci_imp.as_matrix() # use everything else to predict\n",
    "# X = df[feature_names].as_matrix()\n",
    "# y = df[target_name].as_matrix()\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "lasso = Lasso()\n",
    "visualizer = PredictionError(lasso)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "viz = FeatureImportances(GradientBoostingClassifier(), ax=ax)\n",
    "viz.fit(X, y)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREDIT TO : http://www.scikit-yb.org/en/latest/api/classifier/classification_report.html\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the classification data set\n",
    "# data = load_data(\"occupancy\")\n",
    "\n",
    "# Specify the features of interest and the classes of the target\n",
    "# features = [\n",
    "#     \"temperature\", \"relative humidity\", \"light\", \"C02\", \"humidity\"\n",
    "# ]\n",
    "classes = [\"FAST\",\"MEDIUM\",\"NON_URGENT\",\"LESS_URGENT\"]\n",
    "\n",
    "# # Extract the numpy arrays from the data frame\n",
    "# X = data[features].as_matrix()\n",
    "# y = data.occupancy.as_matrix()\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "bayes = GaussianNB()\n",
    "visualizer = ClassificationReport(bayes, classes=classes, support=True)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(model, classes=[0,1,2,3])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(model, classes=[0,1,2,3])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print(svm_clf.coef_)\n",
    "weights = pd.Series(svm_clf.coef_[0],index=inci_imp.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = inci_imp.iloc[train_indices] # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:]\n",
    "\n",
    "df_support['Responsetime_cat'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "inci_imp['Responsetime_cat'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.tools.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['Responsetime_cat'])\n",
    "df_grouped = inci_imp.groupby(['Responsetime_cat'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['Victim_Age','Arrest_status','IsMale','Social_crime_score']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Fast','Medium','Slow', 'Other'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Fast','Medium','Slow', 'Other'])\n",
    "    plt.title(v+' (Original)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression vs. Support Vector Machine Models - Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vivek to work on this section\n",
    "\n",
    "###### To be deleted \n",
    "\n",
    "###### [10 points] Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "Logistic Regresssion is used in many fields of application in which the response variable is binary. Regression models for binary responses, therefore are used to describe probabilities as functions of explanatory variables. It explain a function (logarithm of odds) of mean or probability rather than the mean itself. The interpretations of logistic regression coefficients are made in terms of statemens about odds and odds ratios.\n",
    "## Support Vector Machine\n",
    "This technique has its roots in statistical learning theory and has shown promision empirical results in many practical applications like text categorization. It works well with high dimensional data and avoids the curse of dimenstionality problem. Another unique aspect of this approach is that it represents the decision boundary using a subset of the training examples known as the **Support Vectors**. \n",
    "### Hyperplane\n",
    "In a p-dimensional space, a hyperplane is a flat affine subspace of dimension p−1. For instance, in two dimensions, a hyperplane is a line. In three dimensions, a hyperplane is a flat two-dimensional subspace—that is, a plane. In p > 3 dimensions, it can be hard to visualize a hyperplane, but the notion of a (p − 1)-dimensional flat subspace still applies.\n",
    "\n",
    "## Superior Prediction Accuracy\n",
    "While Logistic Regression fits a model as well as possible using a Training data set, and therefore leading to a substantial number of misclassified patients when applied prospectively on a Test data set. Logistic regression cannot identify possible non-linear structures in the test data set. \n",
    "\n",
    "SVM tries to generalize well when building a model using the given Training data set. In SVM, the generalization performance is optimized by minimizing the classification error on the training set together with minimizing the complexity of the model. This trade-off is called the regularization parameter. SVM can handled more complex decision boundaries (read non-linear). Most importantly, the SVM hs means to prevent the model from being sensitive to outliers in the data, resulting in a model that is capable of making good predictions for prospective analysis.\n",
    "\n",
    "## Superior Efficiency\n",
    "\n",
    "\n",
    "##### Source: \n",
    "* Introduction to Data Mining - Tan, Steinbach and Kumar - Chapter 5\n",
    "* ISLR Sixth Print Edition - Casella, Fienberg, Olkin - Chapter 9\n",
    "* MSDS-6372 Class Slides \n",
    "* The Statistical Sleuth, 3rd Edition - Ramsey, Schafer - Chapter 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Interpreting feature importance using weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be worked on as a team after model is finalized\n",
    "\n",
    "[30 points] Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Data Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be worked on as a team after model is finalized\n",
    "\n",
    "[10 points] Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model— then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR. DREW'S EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Jake Drew's Notebook\n",
    "\n",
    "@misc{BelkNCEARepo,\n",
    "     author = {Drew, J.},\n",
    "     title = {The Belk Endowment Educational Attainment Data Repository for North Carolina Public Schools},\n",
    "     year = {2018},\n",
    "     publisher = {GitHub},\n",
    "     journal = {GitHub repository},\n",
    "     howpublished = {\\url{https://github.com/jakemdrew/EducationDataNC}}\n",
    "}\n",
    "\n",
    "https://github.com/jakemdrew/EducationDataNC/blob/master/Other%20Projects/iPython%20Notebooks/Machine%20Learning/Graduation%20Rates%20February%202018%20-%204%20Years%20Expanded.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x explanatory and y response variables for regression\n",
    "Y = inci_imp['Responsetime_cat']\n",
    "X = inci_imp.drop(['Responsetime_cat'], axis=1)\n",
    "\n",
    "\n",
    "#inspect data \n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "Cross validation is performed using repeated holdout using ShuffleSplit()\n",
    "\n",
    "- Ten folds are used\n",
    "- The split is: 90% training data and 10% test data\n",
    "- A random seed is set so the same random test and training splits are used each time cross validation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into test and training splits\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Scorers for Evaluating Regression Models\n",
    "All regression models created in this notebook are validated using the following metrics:\n",
    "\n",
    "Mean Absolute Error (MAE)\n",
    "Root Mean Squared Error (RMSE) - https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "Mean Absolute Percentage Error (MAPE) - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "For details on making scorers to return multiple mean error scores see:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "https://github.com/scikit-learn/scikit-learn/pull/7388\n",
    "https://github.com/drorata/multiscorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mean absolute error (MAE) to score the regression models created \n",
    "#(the scale of MAE is identical to the response variable)\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "\n",
    "#Function for Root mean squared error\n",
    "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "def rmse(y_actual, y_predicted):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0\n",
    "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "\n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                'MAPE': mape_scorer\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Regression Model Evaluation\n",
    "All regression models are evaluated using the regression model evaluation function below:\n",
    "\n",
    "The following regression evaluation function uses the cross validation object and the custom scorers in the two cells above in combination with sklearn.model_selection's cross_validate function to perform cross validation for regression estimators.\n",
    "The cross validation object above uses a random seed to ensure that all regression estimators are tested on the same randomly selected records for each cross validation fold.\n",
    "Custom scorers are created using the three chosen mean error scores and passed into cross_validate(), so all three scores are calcualted using a single call to cross_validate().\n",
    "All of this functionality is wrapped within the custom EvaluateRegressionEstimator() function below so multiple regression models may be tested using the same test / train cv data and evaluation scores producing a consistent output for each model without the need to re-write the same code over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateRegressionEstimator(regEstimator, X, y, cv):\n",
    "    \n",
    "    scores = cross_validate(regEstimator, X, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
    "\n",
    "    #cross val score sign-flips the outputs of MAE\n",
    "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
    "    scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
    "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    maeAvg = scores['test_MAE'].mean()\n",
    "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
    "    print(print_str.format(maeAvg=maeAvg))\n",
    "\n",
    "    #print mean test_MAPE for all folds\n",
    "    scores['test_MAPE'] = scores['test_MAPE']\n",
    "    mape_avg = scores['test_MAPE'].mean()\n",
    "    print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
    "    print(print_str.format(mape_avg=mape_avg))\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    RMSEavg = scores['test_RMSE'].mean()\n",
    "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
    "    print(print_str.format(RMSEavg=RMSEavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['MAE'] = scores['test_MAE']\n",
    "    scoresResults['MAPE'] = scores['test_MAPE']\n",
    "    scoresResults['RMSE'] = scores['test_RMSE']\n",
    "    return scoresResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses Support Vector Machines for regression of continuous variables (SVR). Please see documentation here:\"\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear regression object and perform a grid search to find the best parameters\n",
    "from sklearn.svm import SVR\n",
    "reg = SVR()\n",
    "\n",
    "#Set up SVR parameters to test (WARNING: Creates 320 models!!!) \n",
    "costs = [0.001, 0.1, 1, 10]\n",
    "defGamma = 1 / X.shape[1]  #This is the default value for the gamma parameter\n",
    "gammas = [defGamma, 0.1, 1, 10]\n",
    "kernels = ['rbf','linear']\n",
    "parameters = {'C': costs, 'gamma' : gammas, 'kernel': kernels}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "                   kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "EvaluateRegressionEstimator(regEstimator, X, Y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "                   kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "regEstimator.fit(X, Y)\n",
    "yhat = regEstimator.predict(X)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses a custom Logit model based on transformations of the Linear Regression object. Please see documentation here:\"\n",
    "\n",
    "https://stackoverflow.com/questions/33845539/modelling-probabilities-in-a-regularized-logistic-regression-model-in-python\n",
    "https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class LogitRegression(LinearRegression):\n",
    "\n",
    "    def fit(self, x, p):\n",
    "        p[p==0] = 0.009    #0.1111111111111111 \n",
    "        p[p==1] = 0.991    #0.9999999999999999  big precision seems to kill MAE scores here?\n",
    "        #e = 0.0000000000000001\n",
    "        #p = p * e + 0.5 * e                    This technique was really bad too. \n",
    "        p = np.asarray(p)\n",
    "        y = np.log(p / (1 - p))\n",
    "        return super(LogitRegression, self).fit(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y = super(LogitRegression, self).predict(x)\n",
    "        yhat = 1 / (np.exp(-y) + 1)\n",
    "        yhat[yhat <= 0.009] = 0\n",
    "        yhat[yhat >= 0.991] = 1\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert y to a proability \n",
    "Y = Y / 100\n",
    "\n",
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "linreg = LogitRegression()\n",
    "parameters = {'normalize':(True,False), 'fit_intercept':(True,False)}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   #, n_jobs=8 # jobs to run in parallel (This breaks the custom estimators for some reason!)\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CappedLinearRegression predictions between 0 and 100% using the best parameters for our Linear Regression object\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateRegressionEstimator(regEstimator, X, Y, cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
