{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DALLAS CRIME DATA</center></h1>\n",
    "<h2><center>MSDS 7331 Mini Project</center></h2>\n",
    "<h2><center>SVM and Logistic Regression Modeling</center></h2>\n",
    "\n",
    "<h4><center>Team Members</center></h4>\n",
    "\n",
    "           Yejur Singh Kunwar           Bin Yu               Vivek Viswanathan          Kevin Mendonsa\n",
    "              Dallas, TX               Dallas, TX               Dallas, TX                Irvine, CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We will be using Dallas crime datset where police incidents were recorded. For this miniproject, main dataframe that we will be utilizing is the saved dataframe from LAB 1, details for this dataset can be found in LAB 1.  \n",
    "\n",
    "The data shape was 255154 observations with 44 attributes.\n",
    "\n",
    "- Logistic Regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Logistic Regression Model vs. Support Vector Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be deleted\n",
    "\n",
    "[50 points] Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing necessary packages for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary packages for python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "from dateutil.relativedelta import relativedelta\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "from itertools import cycle, islice\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import metrics as mt\n",
    "from __future__ import print_function, division\n",
    "from patsy import dmatrices, dmatrix\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data/\" # Generic path\n",
    "incident = pd.read_csv(path + 'LAB1_completed_Dataset_clean.csv', low_memory= False)\n",
    "#-------------------------------\n",
    "# LOADING SUPPORTING DATA SETS\n",
    "#-------------------------------\n",
    "# Dallas area population dataset\n",
    "#-------------------------------\n",
    "population = pd.read_csv(path + 'Population.csv', low_memory=False)\n",
    "#--------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCR_Offense_Name change this\n",
    "\n",
    "incident.loc[:,'UCR_Offense_Name'] = incident['UCR_Offense_Name'].fillna(\"MISSING\")\n",
    "\n",
    "THEFT_FRAUD     = dict.fromkeys(['THEFT/BMV', 'THEFT ORG RETAIL', 'BURGLARY-RESIDENCE', 'OTHER THEFTS',\n",
    "                                 'ROBBERY-INDIVIDUAL','THEFT/SHOPLIFT', 'BURGLARY-BUSINESS', 'FORGE & COUNTERFEIT', \n",
    "                                 'FRAUD', 'EMBEZZLEMENT','ROBBERY-BUSINESS','THEFT ORG RETAIL'],\"THEFT_FRAUD\" ) \n",
    "MVA_TRAFFIC      =dict.fromkeys(['ACCIDENT MV', 'MOTOR VEHICLE ACCIDENT', 'UUMV', 'TRAFFIC VIOLATION',\n",
    "                                 'TRAFFIC FATALITY'],\"MVA_TRAFFIC\" )        \n",
    "WEAPONS_FIREARMS =dict.fromkeys(['WEAPONS', 'ARSON', 'INJURED FIREARM'], \"WEAPONS_FIREARMS\")         \n",
    "ASSUALT          = dict.fromkeys(['ASSAULT','VANDALISM & CRIM MISCHIEF', 'AGG ASSAULT - NFV', 'OFFENSE AGAINST CHILD',\n",
    "                                  'AGG ASSAULT - FV'], \"ASSUALT\")\n",
    "OTHERS_THREATS   = dict.fromkeys(['FOUND', 'OTHERS', 'LOST', 'CRIMINAL TRESPASS', 'DISORDERLY CONDUCT', \n",
    "                                  'ANIMAL BITE','INJURED HOME','INJURED PUBLIC', 'TERRORISTIC THREAT', \n",
    "                                  'EVADING', 'INJURED OCCUPA', 'ORANIZED CRIME', 'KIDNAPPING', \n",
    "                                  'RESIST ARREST','FAIL TO ID', 'HUMAN TRAFFICKING', 'MISSING'], \"OTHERS_THREATS\")\n",
    "INTOXICATION     = dict.fromkeys(['DRUNK & DISORDERLY', 'DWI', 'NARCOTICS & DRUGS', 'LIQUOR OFFENSE', \n",
    "                                  'INTOXICATION MANSLAUGHTER'],\"INTOXICATION\")\n",
    "MURDER_DEATH     = dict.fromkeys(['SUDDEN DEATH&FOUND BODIES','MURDER'], \"MURDER_DEATH\")\n",
    "                    \n",
    "\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(THEFT_FRAUD)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(MVA_TRAFFIC)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(WEAPONS_FIREARMS)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(ASSUALT)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(OTHERS_THREATS)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(INTOXICATION)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(MURDER_DEATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FILTERING OUT UNNECESSARY NULL DATA\n",
    "incident = incident[incident['Watch']!=0]\n",
    "incident = incident[(incident['Victim_Age']>=0) & (incident['Victim_Age']<=90)]\n",
    "incident = incident[incident['Victim_Race']!=\"Unknown\"]\n",
    "incident = incident[incident['Victim_Type']!=\"Unknown\"]\n",
    "incident= incident[incident.Number_of_offense != \"RP\"]\n",
    "incident.loc[:,'Social_crime_score'] = incident['Hate_Crime']+incident['Gang_Related_Offense']+incident['Drug_Related']\n",
    "incident = incident[incident['Victim_Gender']!=\"U\"]\n",
    "incident.loc[:,'IsMale'] = incident.Victim_Gender=='M' \n",
    "incident.IsMale = incident.IsMale.astype(np.int)\n",
    "incident.loc[:,'Number_of_offense']= incident.Number_of_offense.astype(np.int)\n",
    "\n",
    "incident.loc[:,'Victim_Age'] = incident['Victim_Age'].astype(np.int)\n",
    "\n",
    "incident.loc[:,'Response_time'] = incident['Response_time'].fillna(incident['Response_time'].mean()).astype(np.int)\n",
    "incident.loc[:,'Responsetime_cat'] = pd.cut(incident.Response_time,[0,15,30,60,97],4,labels=[0,1,2,3])\n",
    "\n",
    "# incident= incident[incident['Responsetime_cat']!=np.nan]\n",
    "\n",
    "incident['UCR_Offense_Name'] = pd.Categorical(incident['UCR_Offense_Name']).codes\n",
    "incident['Day1_of_the_Week'] = pd.Categorical(incident['Day1_of_the_Week']).codes\n",
    "incident['Division'] = pd.Categorical(incident['Division']).codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS\n",
    "\n",
    "#### Preparation of Data\n",
    "\n",
    "The incident dataset is further cleaned and and prepared in right format to feed into model algorithm. \n",
    "\n",
    "The attributes where null or empty values were replaced with Unknows or other category is removed in order to reduce data ambuigity in the model for selected attributes. \n",
    "\n",
    "The data is splitted to response variable (response_category_classification) which will be called \"Y\" and explanatory variables or attributes \"X\" for placing it into the algoirith model. \n",
    "\n",
    "The main dataset is splitted into training and test dataset by 80/20 split method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inci_imp = incident[['Watch', 'Day1_of_the_Week', 'Victim_Age',  'UCR_Offense_Name','Number_of_offense', \n",
    "#                      'Response_time',\n",
    "                     'Responsetime_cat','Arrest_status','Social_crime_score', 'IsMale']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Watch                 0\n",
       "Day1_of_the_Week      0\n",
       "Victim_Age            0\n",
       "UCR_Offense_Name      0\n",
       "Number_of_offense     0\n",
       "Responsetime_cat      0\n",
       "Arrest_status         0\n",
       "Social_crime_score    0\n",
       "IsMale                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inci_imp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING AND TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Watch', 'Day1_of_the_Week', 'Victim_Age', 'UCR_Offense_Name',\n",
       "       'Number_of_offense', 'Responsetime_cat', 'Arrest_status',\n",
       "       'Social_crime_score', 'IsMale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inci_imp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Responsetime_cat' in inci_imp:\n",
    "    y = inci_imp['Responsetime_cat'].values # get the labels we want\n",
    "    del inci_imp['Responsetime_cat'] # get rid of the class label\n",
    "    X = inci_imp.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCR_Offense_Name    Arrest_status        -0.198657\n",
      "                    Watch                -0.081682\n",
      "Victim_Age          Number_of_offense    -0.041968\n",
      "Watch               IsMale               -0.035694\n",
      "Victim_Age          Arrest_status        -0.032141\n",
      "Social_crime_score  Victim_Age           -0.029758\n",
      "Number_of_offense   UCR_Offense_Name     -0.027226\n",
      "Social_crime_score  UCR_Offense_Name     -0.021431\n",
      "Day1_of_the_Week    Number_of_offense    -0.010674\n",
      "IsMale              Day1_of_the_Week     -0.009545\n",
      "                                            ...   \n",
      "Victim_Age          UCR_Offense_Name      0.019870\n",
      "Day1_of_the_Week    UCR_Offense_Name      0.020790\n",
      "Social_crime_score  Watch                 0.025329\n",
      "                    IsMale                0.029397\n",
      "Number_of_offense   Social_crime_score    0.046538\n",
      "Arrest_status       Social_crime_score    0.048808\n",
      "UCR_Offense_Name    IsMale                0.055583\n",
      "IsMale              Victim_Age            0.060202\n",
      "Arrest_status       Watch                 0.063013\n",
      "Number_of_offense   Arrest_status         0.068401\n",
      "Length: 28, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Create correlation matrix\n",
    "CorrMatrix = inci_imp.corr()\n",
    "\n",
    "# Highest Correlation Pairs\n",
    "corrPairs = CorrMatrix.unstack().sort_values(kind=\"quicksort\")\n",
    "#- REMOVE DUPLICATES\n",
    "corrPairs = corrPairs[::2]\n",
    "corrPairs = corrPairs[corrPairs.index.get_level_values(0) != corrPairs.index.get_level_values(1)]\n",
    "with pd.option_context('display.max_rows',20):\n",
    "    print(corrPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Logistic Regression\n",
    "Now let's use Logistic Regression from `scikit-learn`. The documentation can be found here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.5774703753284743\n",
      "confusion matrix\n",
      " [[23294     0     0     0]\n",
      " [ 6751     0     0     0]\n",
      " [ 6303     0     0     0]\n",
      " [ 3990     0     0     0]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.5801973325400367\n",
      "confusion matrix\n",
      " [[23404     0     0     0]\n",
      " [ 6683     0     0     0]\n",
      " [ 6387     0     0     0]\n",
      " [ 3864     0     0     0]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.5757102484010115\n",
      "confusion matrix\n",
      " [[23223     0     0     0]\n",
      " [ 6807     0     0     0]\n",
      " [ 6337     0     0     0]\n",
      " [ 3971     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.5793048738162526\n",
      "confusion matrix\n",
      " [[23368     0     0     0]\n",
      " [ 6720     0     0     0]\n",
      " [ 6335     0     0     0]\n",
      " [ 3915     0     0     0]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.574520303435966\n",
      "confusion matrix\n",
      " [[23175     0     0     0]\n",
      " [ 6915     0     0     0]\n",
      " [ 6382     0     0     0]\n",
      " [ 3866     0     0     0]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.5812633249045565\n",
      "confusion matrix\n",
      " [[23447     0     0     0]\n",
      " [ 6672     0     0     0]\n",
      " [ 6379     0     0     0]\n",
      " [ 3840     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# this does the exact same thing as the above block of code, but with shorter syntax\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "take_nd() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1fd830a3a61f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# and here is an even shorter way of getting the accuracies for each training and test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0maccuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv_object\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# this also can help with parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             return_times=True)\n\u001b[1;32m--> 206\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[1;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0my_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0my_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36msafe_indexing\u001b[1;34m(X, indices)\u001b[0m\n\u001b[0;32m    158\u001b[0m                                    indices.dtype.kind == 'i'):\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# This is often substantially faster than X[indices]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: take_nd() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "# and here is an even shorter way of getting the accuracies for each training and test set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d7547e60374fb8a1f9ed7ba8ab470b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.451, description='cost', max=5.0, min=0.001, step=0.05), Output()), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.lr_explor(cost)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None) # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = inci_imp.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,inci_imp.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=inci_imp.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more improvement and guarding against overfitting: At this point it would make sense to remove variables that are highly related to one another or ones that are irrelevant and keep going with the weights analysis. What variables would you remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': inci_imp.columns,\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load the data\n",
    "# df = load_data('concrete')\n",
    "# feature_names = ['cement', 'slag', 'ash', 'water', 'splast', 'coarse', 'fine', 'age']\n",
    "# target_name = 'strength'\n",
    "\n",
    "# Get the X and y data from the DataFrame\n",
    "\n",
    "if 'Responsetime_cat' in inci_imp:\n",
    "    y = inci_imp['Responsetime_cat'].as_matrix() # get the labels we want\n",
    "    del inci_imp['Responsetime_cat'] # get rid of the class label\n",
    "    X = inci_imp.as_matrix() # use everything else to predict\n",
    "# X = df[feature_names].as_matrix()\n",
    "# y = df[target_name].as_matrix()\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "lasso = Lasso()\n",
    "visualizer = PredictionError(lasso)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "viz = FeatureImportances(GradientBoostingClassifier(), ax=ax)\n",
    "viz.fit(X, y)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREDIT TO : http://www.scikit-yb.org/en/latest/api/classifier/classification_report.html\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the classification data set\n",
    "# data = load_data(\"occupancy\")\n",
    "\n",
    "# Specify the features of interest and the classes of the target\n",
    "# features = [\n",
    "#     \"temperature\", \"relative humidity\", \"light\", \"C02\", \"humidity\"\n",
    "# ]\n",
    "classes = [\"FAST\",\"MEDIUM\",\"NON_URGENT\",\"LESS_URGENT\"]\n",
    "\n",
    "# # Extract the numpy arrays from the data frame\n",
    "# X = data[features].as_matrix()\n",
    "# y = data.occupancy.as_matrix()\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "bayes = GaussianNB()\n",
    "visualizer = ClassificationReport(bayes, classes=classes, support=True)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(model, classes=[0,1,2,3])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(model, classes=[0,1,2,3])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print(svm_clf.coef_)\n",
    "weights = pd.Series(svm_clf.coef_[0],index=inci_imp.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = inci_imp.iloc[train_indices] # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:]\n",
    "\n",
    "df_support['Responsetime_cat'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "inci_imp['Responsetime_cat'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.tools.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['Responsetime_cat'])\n",
    "df_grouped = inci_imp.groupby(['Responsetime_cat'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['Victim_Age','Arrest_status','IsMale','Social_crime_score']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Fast','Medium','Slow', 'Other'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Fast','Medium','Slow', 'Other'])\n",
    "    plt.title(v+' (Original)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression vs. Support Vector Machine Models - Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vivek to work on this section\n",
    "\n",
    "###### To be deleted \n",
    "\n",
    "###### [10 points] Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "Logistic Regresssion is used in many fields of application in which the response variable is binary. Regression models for binary responses, therefore are used to describe probabilities as functions of explanatory variables. It explain a function (logarithm of odds) of mean or probability rather than the mean itself. The interpretations of logistic regression coefficients are made in terms of statemens about odds and odds ratios.\n",
    "## Support Vector Machine\n",
    "This technique has its roots in statistical learning theory and has shown promision empirical results in many practical applications like text categorization. It works well with high dimensional data and avoids the curse of dimenstionality problem. Another unique aspect of this approach is that it represents the decision boundary using a subset of the training examples known as the **Support Vectors**. \n",
    "### Hyperplane\n",
    "In a p-dimensional space, a hyperplane is a flat affine subspace of dimension pâˆ’1. For instance, in two dimensions, a hyperplane is a line. In three dimensions, a hyperplane is a flat two-dimensional subspaceâ€”that is, a plane. In p > 3 dimensions, it can be hard to visualize a hyperplane, but the notion of a (p âˆ’ 1)-dimensional flat subspace still applies.\n",
    "\n",
    "## Superior Prediction Accuracy\n",
    "While Logistic Regression fits a model as well as possible using a Training data set, and therefore leading to a substantial number of misclassified patients when applied prospectively on a Test data set. Logistic regression cannot identify possible non-linear structures in the test data set. \n",
    "\n",
    "SVM tries to generalize well when building a model using the given Training data set. In SVM, the generalization performance is optimized by minimizing the classification error on the training set together with minimizing the complexity of the model. This trade-off is called the regularization parameter. SVM can handled more complex decision boundaries (read non-linear). Most importantly, the SVM hs means to prevent the model from being sensitive to outliers in the data, resulting in a model that is capable of making good predictions for prospective analysis.\n",
    "\n",
    "## Superior Efficiency\n",
    "\n",
    "\n",
    "##### Source: \n",
    "* Introduction to Data Mining - Tan, Steinbach and Kumar - Chapter 5\n",
    "* ISLR Sixth Print Edition - Casella, Fienberg, Olkin - Chapter 9\n",
    "* MSDS-6372 Class Slides \n",
    "* The Statistical Sleuth, 3rd Edition - Ramsey, Schafer - Chapter 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "## Interpreting feature importance using weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be worked on as a team after model is finalized\n",
    "\n",
    "[30 points] Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Data Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be worked on as a team after model is finalized\n",
    "\n",
    "[10 points] Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC modelâ€” then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR. DREW'S EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Jake Drew's Notebook\n",
    "\n",
    "@misc{BelkNCEARepo,\n",
    "     author = {Drew, J.},\n",
    "     title = {The Belk Endowment Educational Attainment Data Repository for North Carolina Public Schools},\n",
    "     year = {2018},\n",
    "     publisher = {GitHub},\n",
    "     journal = {GitHub repository},\n",
    "     howpublished = {\\url{https://github.com/jakemdrew/EducationDataNC}}\n",
    "}\n",
    "\n",
    "https://github.com/jakemdrew/EducationDataNC/blob/master/Other%20Projects/iPython%20Notebooks/Machine%20Learning/Graduation%20Rates%20February%202018%20-%204%20Years%20Expanded.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x explanatory and y response variables for regression\n",
    "Y = inci_imp['Responsetime_cat']\n",
    "X = inci_imp.drop(['Responsetime_cat'], axis=1)\n",
    "\n",
    "\n",
    "#inspect data \n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "Cross validation is performed using repeated holdout using ShuffleSplit()\n",
    "\n",
    "- Ten folds are used\n",
    "- The split is: 90% training data and 10% test data\n",
    "- A random seed is set so the same random test and training splits are used each time cross validation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into test and training splits\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Scorers for Evaluating Regression Models\n",
    "All regression models created in this notebook are validated using the following metrics:\n",
    "\n",
    "Mean Absolute Error (MAE)\n",
    "Root Mean Squared Error (RMSE) - https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "Mean Absolute Percentage Error (MAPE) - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "For details on making scorers to return multiple mean error scores see:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "https://github.com/scikit-learn/scikit-learn/pull/7388\n",
    "https://github.com/drorata/multiscorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mean absolute error (MAE) to score the regression models created \n",
    "#(the scale of MAE is identical to the response variable)\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "\n",
    "#Function for Root mean squared error\n",
    "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "def rmse(y_actual, y_predicted):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0\n",
    "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "\n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                'MAPE': mape_scorer\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Regression Model Evaluation\n",
    "All regression models are evaluated using the regression model evaluation function below:\n",
    "\n",
    "The following regression evaluation function uses the cross validation object and the custom scorers in the two cells above in combination with sklearn.model_selection's cross_validate function to perform cross validation for regression estimators.\n",
    "The cross validation object above uses a random seed to ensure that all regression estimators are tested on the same randomly selected records for each cross validation fold.\n",
    "Custom scorers are created using the three chosen mean error scores and passed into cross_validate(), so all three scores are calcualted using a single call to cross_validate().\n",
    "All of this functionality is wrapped within the custom EvaluateRegressionEstimator() function below so multiple regression models may be tested using the same test / train cv data and evaluation scores producing a consistent output for each model without the need to re-write the same code over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateRegressionEstimator(regEstimator, X, y, cv):\n",
    "    \n",
    "    scores = cross_validate(regEstimator, X, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
    "\n",
    "    #cross val score sign-flips the outputs of MAE\n",
    "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
    "    scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
    "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    maeAvg = scores['test_MAE'].mean()\n",
    "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
    "    print(print_str.format(maeAvg=maeAvg))\n",
    "\n",
    "    #print mean test_MAPE for all folds\n",
    "    scores['test_MAPE'] = scores['test_MAPE']\n",
    "    mape_avg = scores['test_MAPE'].mean()\n",
    "    print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
    "    print(print_str.format(mape_avg=mape_avg))\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    RMSEavg = scores['test_RMSE'].mean()\n",
    "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
    "    print(print_str.format(RMSEavg=RMSEavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['MAE'] = scores['test_MAE']\n",
    "    scoresResults['MAPE'] = scores['test_MAPE']\n",
    "    scoresResults['RMSE'] = scores['test_RMSE']\n",
    "    return scoresResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses Support Vector Machines for regression of continuous variables (SVR). Please see documentation here:\"\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear regression object and perform a grid search to find the best parameters\n",
    "from sklearn.svm import SVR\n",
    "reg = SVR()\n",
    "\n",
    "#Set up SVR parameters to test (WARNING: Creates 320 models!!!) \n",
    "costs = [0.001, 0.1, 1, 10]\n",
    "defGamma = 1 / X.shape[1]  #This is the default value for the gamma parameter\n",
    "gammas = [defGamma, 0.1, 1, 10]\n",
    "kernels = ['rbf','linear']\n",
    "parameters = {'C': costs, 'gamma' : gammas, 'kernel': kernels}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "                   kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "EvaluateRegressionEstimator(regEstimator, X, Y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "                   kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "regEstimator.fit(X, Y)\n",
    "yhat = regEstimator.predict(X)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses a custom Logit model based on transformations of the Linear Regression object. Please see documentation here:\"\n",
    "\n",
    "https://stackoverflow.com/questions/33845539/modelling-probabilities-in-a-regularized-logistic-regression-model-in-python\n",
    "https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class LogitRegression(LinearRegression):\n",
    "\n",
    "    def fit(self, x, p):\n",
    "        p[p==0] = 0.009    #0.1111111111111111 \n",
    "        p[p==1] = 0.991    #0.9999999999999999  big precision seems to kill MAE scores here?\n",
    "        #e = 0.0000000000000001\n",
    "        #p = p * e + 0.5 * e                    This technique was really bad too. \n",
    "        p = np.asarray(p)\n",
    "        y = np.log(p / (1 - p))\n",
    "        return super(LogitRegression, self).fit(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y = super(LogitRegression, self).predict(x)\n",
    "        yhat = 1 / (np.exp(-y) + 1)\n",
    "        yhat[yhat <= 0.009] = 0\n",
    "        yhat[yhat >= 0.991] = 1\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert y to a proability \n",
    "Y = Y / 100\n",
    "\n",
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "linreg = LogitRegression()\n",
    "parameters = {'normalize':(True,False), 'fit_intercept':(True,False)}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   #, n_jobs=8 # jobs to run in parallel (This breaks the custom estimators for some reason!)\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CappedLinearRegression predictions between 0 and 100% using the best parameters for our Linear Regression object\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateRegressionEstimator(regEstimator, X, Y, cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
