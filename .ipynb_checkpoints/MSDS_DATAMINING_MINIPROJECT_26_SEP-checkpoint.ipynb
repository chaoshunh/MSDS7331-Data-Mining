{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DALLAS CRIME DATA</center></h1>\n",
    "<h2><center>MSDS 7331 Mini Lab 1</center></h2>\n",
    "<h3><center>SVM and Logistic Regression Modeling</center></h3>\n",
    "\n",
    "<h4><center>Team Members</center></h4>\n",
    "\n",
    "           Yejur Singh Kunwar           Bin Yu               Vivek Viswanathan          Kevin Mendonsa\n",
    "              Dallas, TX               Dallas, TX               Dallas, TX                Irvine, CA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We will be using Dallas crime datset where police incidents were recorded. For this miniproject, main dataframe that we will be utilizing is the saved dataframe from LAB 1, details for this dataset can be found in LAB 1.  \n",
    "\n",
    "The data shape was 255154 observations with 44 attributes.\n",
    "\n",
    "- Logistic Regression\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be Deleted\n",
    "\n",
    "[50 points] Create a logistic regression model and a support vector machine model for the\n",
    "classification task involved with your dataset. Assess how well each model performs (use\n",
    "80/20 training/testing split for your data). Adjust parameters of the models to make them\n",
    "more accurate. If your dataset size requires the use of stochastic gradient descent, then\n",
    "linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic\n",
    "regression and linear support vector machines. For many problems, SGD will be required in\n",
    "order to train the SVM model in a reasonable timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing necessary packages for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary packages for python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import string\n",
    "from dateutil.relativedelta import relativedelta\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "from itertools import cycle, islice\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn import metrics as mt\n",
    "from __future__ import print_function, division\n",
    "from patsy import dmatrices, dmatrix\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../Data/\" # Generic path\n",
    "incident = pd.read_csv(path+'LAB1_completed_Dataset_clean.csv', low_memory= False)\n",
    "#-------------------------------\n",
    "# LOADING SUPPORTING DATA SETS\n",
    "#-------------------------------\n",
    "# Dallas area population dataset\n",
    "#-------------------------------\n",
    "population = pd.read_csv(path + 'Population.csv', low_memory=False)\n",
    "#--------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCR_Offense_Name change this\n",
    "\n",
    "incident.loc[:,'UCR_Offense_Name'] = incident['UCR_Offense_Name'].fillna(\"MISSING\")\n",
    "\n",
    "THEFT_FRAUD     = dict.fromkeys(['THEFT/BMV', 'THEFT ORG RETAIL', 'BURGLARY-RESIDENCE', 'OTHER THEFTS',\n",
    "                                 'ROBBERY-INDIVIDUAL','THEFT/SHOPLIFT', 'BURGLARY-BUSINESS', 'FORGE & COUNTERFEIT', \n",
    "                                 'FRAUD', 'EMBEZZLEMENT','ROBBERY-BUSINESS','THEFT ORG RETAIL'],\"THEFT_FRAUD\" ) \n",
    "MVA_TRAFFIC      =dict.fromkeys(['ACCIDENT MV', 'MOTOR VEHICLE ACCIDENT', 'UUMV', 'TRAFFIC VIOLATION',\n",
    "                                 'TRAFFIC FATALITY'],\"MVA_TRAFFIC\" )        \n",
    "WEAPONS_FIREARMS =dict.fromkeys(['WEAPONS', 'ARSON', 'INJURED FIREARM'], \"WEAPONS_FIREARMS\")         \n",
    "ASSUALT          = dict.fromkeys(['ASSAULT','VANDALISM & CRIM MISCHIEF', 'AGG ASSAULT - NFV', 'OFFENSE AGAINST CHILD',\n",
    "                                  'AGG ASSAULT - FV'], \"ASSUALT\")\n",
    "OTHERS_THREATS   = dict.fromkeys(['FOUND', 'OTHERS', 'LOST', 'CRIMINAL TRESPASS', 'DISORDERLY CONDUCT', \n",
    "                                  'ANIMAL BITE','INJURED HOME','INJURED PUBLIC', 'TERRORISTIC THREAT', \n",
    "                                  'EVADING', 'INJURED OCCUPA', 'ORANIZED CRIME', 'KIDNAPPING', \n",
    "                                  'RESIST ARREST','FAIL TO ID', 'HUMAN TRAFFICKING', 'MISSING'], \"OTHERS_THREATS\")\n",
    "INTOXICATION     = dict.fromkeys(['DRUNK & DISORDERLY', 'DWI', 'NARCOTICS & DRUGS', 'LIQUOR OFFENSE', \n",
    "                                  'INTOXICATION MANSLAUGHTER'],\"INTOXICATION\")\n",
    "MURDER_DEATH     = dict.fromkeys(['SUDDEN DEATH&FOUND BODIES','MURDER'], \"MURDER_DEATH\")\n",
    "                    \n",
    "\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(THEFT_FRAUD)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(MVA_TRAFFIC)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(WEAPONS_FIREARMS)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(ASSUALT)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(OTHERS_THREATS)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(INTOXICATION)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(MURDER_DEATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FILTERING OUT UNNECESSARY NULL DATA\n",
    "incident = incident[incident['Watch']!=0]\n",
    "incident = incident[(incident['Victim_Age']>=0) & (incident['Victim_Age']<=90)]\n",
    "incident = incident[incident['Victim_Race']!=\"Unknown\"]\n",
    "incident = incident[incident['Victim_Type']!=\"Unknown\"]\n",
    "incident= incident[incident.Number_of_offense != \"RP\"]\n",
    "incident.loc[:,'Social_crime_score'] = incident['Hate_Crime']+incident['Gang_Related_Offense']+incident['Drug_Related']\n",
    "incident = incident[incident['Victim_Gender']!=\"U\"]\n",
    "incident.loc[:,'IsMale'] = incident.Victim_Gender=='M' \n",
    "incident.IsMale = incident.IsMale.astype(np.int)\n",
    "incident.loc[:,'Number_of_offense']= incident.Number_of_offense.astype(np.int)\n",
    "\n",
    "incident.loc[:,'Victim_Age'] = incident['Victim_Age'].astype(np.int)\n",
    "\n",
    "incident.loc[:,'Response_time'] = incident['Response_time'].fillna(incident['Response_time'].mean()).astype(np.int)\n",
    "incident.loc[:,'Responsetime_cat'] = pd.cut(incident.Response_time,[0,15,30,60,97],4,labels=[0,1,2,3])\n",
    "\n",
    "# incident= incident[incident['Responsetime_cat']!=np.nan]\n",
    "\n",
    "incident['UCR_Offense_Name'] = pd.Categorical(incident['UCR_Offense_Name']).codes\n",
    "incident['Day1_of_the_Week'] = pd.Categorical(incident['Day1_of_the_Week']).codes\n",
    "incident['Division'] = pd.Categorical(incident['Division']).codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS\n",
    "\n",
    "#### Preparation of Data\n",
    "\n",
    "The incident dataset is further cleaned and and prepared in right format to feed into model algorithm. \n",
    "\n",
    "The attributes where null or empty values were replaced with Unknows or other category is removed in order to reduce data ambuigity in the model for selected attributes. \n",
    "\n",
    "The data is splitted to response variable (response_category_classification) which will be called \"Y\" and explanatory variables or attributes \"X\" for placing it into the algoirith model. \n",
    "\n",
    "The main dataset is splitted into training and test dataset by 80/20 split method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inci_imp = incident[['Watch', 'Day1_of_the_Week', 'Victim_Age',  'UCR_Offense_Name','Number_of_offense', \n",
    "#                      'Response_time',\n",
    "                     'Responsetime_cat','Arrest_status','Social_crime_score', 'IsMale']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inci_imp.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING AND TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inci_imp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Responsetime_cat' in inci_imp:\n",
    "    y = inci_imp['Responsetime_cat'].values # get the labels we want\n",
    "    del inci_imp['Responsetime_cat'] # get rid of the class label\n",
    "    X = inci_imp.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create correlation matrix\n",
    "CorrMatrix = inci_imp.corr()\n",
    "\n",
    "# Highest Correlation Pairs\n",
    "corrPairs = CorrMatrix.unstack().sort_values(kind=\"quicksort\")\n",
    "#- REMOVE DUPLICATES\n",
    "corrPairs = corrPairs[::2]\n",
    "corrPairs = corrPairs[corrPairs.index.get_level_values(0) != corrPairs.index.get_level_values(1)]\n",
    "with pd.option_context('display.max_rows',20):\n",
    "    print(corrPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Logistic Regression\n",
    "Now let's use Logistic Regression from `scikit-learn`. The documentation can be found here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this does the exact same thing as the above block of code, but with shorter syntax\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # and here is an even shorter way of getting the accuracies for each training and test set\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "# print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None) # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = inci_imp.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,inci_imp.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=inci_imp.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more improvement and guarding against overfitting: At this point it would make sense to remove variables that are highly related to one another or ones that are irrelevant and keep going with the weights analysis. What variables would you remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05) \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': inci_imp.columns,\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load the data\n",
    "# df = load_data('concrete')\n",
    "# feature_names = ['cement', 'slag', 'ash', 'water', 'splast', 'coarse', 'fine', 'age']\n",
    "# target_name = 'strength'\n",
    "\n",
    "# Get the X and y data from the DataFrame\n",
    "\n",
    "if 'Responsetime_cat' in inci_imp:\n",
    "    y = inci_imp['Responsetime_cat'].as_matrix() # get the labels we want\n",
    "    del inci_imp['Responsetime_cat'] # get rid of the class label\n",
    "    X = inci_imp.as_matrix() # use everything else to predict\n",
    "# X = df[feature_names].as_matrix()\n",
    "# y = df[target_name].as_matrix()\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "lasso = Lasso()\n",
    "visualizer = PredictionError(lasso)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "# Create a new matplotlib figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "viz = FeatureImportances(GradientBoostingClassifier(), ax=ax)\n",
    "viz.fit(X, y)\n",
    "viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREDIT TO : http://www.scikit-yb.org/en/latest/api/classifier/classification_report.html\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the classification data set\n",
    "# data = load_data(\"occupancy\")\n",
    "\n",
    "# Specify the features of interest and the classes of the target\n",
    "# features = [\n",
    "#     \"temperature\", \"relative humidity\", \"light\", \"C02\", \"humidity\"\n",
    "# ]\n",
    "classes = [\"FAST\",\"MEDIUM\",\"NON_URGENT\",\"LESS_URGENT\"]\n",
    "\n",
    "# # Extract the numpy arrays from the data frame\n",
    "# X = data[features].as_matrix()\n",
    "# y = data.occupancy.as_matrix()\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "bayes = GaussianNB()\n",
    "visualizer = ClassificationReport(bayes, classes=classes, support=True)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(model, classes=[0,1,2,3])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm = ConfusionMatrix(model, classes=[0,1,2,3])\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print(svm_clf.coef_)\n",
    "weights = pd.Series(svm_clf.coef_[0],index=inci_imp.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = inci_imp.iloc[train_indices] # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:]\n",
    "\n",
    "df_support['Responsetime_cat'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "inci_imp['Responsetime_cat'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.tools.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['Responsetime_cat'])\n",
    "df_grouped = inci_imp.groupby(['Responsetime_cat'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['Victim_Age','Arrest_status','IsMale','Social_crime_score']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Fast','Medium','Slow', 'Other'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Fast','Medium','Slow', 'Other'])\n",
    "    plt.title(v+' (Original)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and SVM - A comparitive view\n",
    "\n",
    "### To be filled by Vivek Viswanthan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be deleted\n",
    "\n",
    "[10 points] Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of Features using weights\n",
    "\n",
    "### To be filled by team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be deleted\n",
    "\n",
    "[30 points] Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insights\n",
    "\n",
    "### To be filled by team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be deleted\n",
    "\n",
    "[10 points] Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model—then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR. DREW'S EXAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dr. Jake Drew's Notebook\n",
    "\n",
    "@misc{BelkNCEARepo,\n",
    "     author = {Drew, J.},\n",
    "     title = {The Belk Endowment Educational Attainment Data Repository for North Carolina Public Schools},\n",
    "     year = {2018},\n",
    "     publisher = {GitHub},\n",
    "     journal = {GitHub repository},\n",
    "     howpublished = {\\url{https://github.com/jakemdrew/EducationDataNC}}\n",
    "}\n",
    "\n",
    "https://github.com/jakemdrew/EducationDataNC/blob/master/Other%20Projects/iPython%20Notebooks/Machine%20Learning/Graduation%20Rates%20February%202018%20-%204%20Years%20Expanded.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x explanatory and y response variables for regression\n",
    "Y = inci_imp['Responsetime_cat']\n",
    "X = inci_imp.drop(['Responsetime_cat'], axis=1)\n",
    "\n",
    "\n",
    "#inspect data \n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "Cross validation is performed using repeated holdout using ShuffleSplit()\n",
    "\n",
    "- Ten folds are used\n",
    "- The split is: 90% training data and 10% test data\n",
    "- A random seed is set so the same random test and training splits are used each time cross validation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into test and training splits\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Scorers for Evaluating Regression Models\n",
    "All regression models created in this notebook are validated using the following metrics:\n",
    "\n",
    "Mean Absolute Error (MAE)\n",
    "Root Mean Squared Error (RMSE) - https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "Mean Absolute Percentage Error (MAPE) - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "For details on making scorers to return multiple mean error scores see:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html\n",
    "https://github.com/scikit-learn/scikit-learn/pull/7388\n",
    "https://github.com/drorata/multiscorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mean absolute error (MAE) to score the regression models created \n",
    "#(the scale of MAE is identical to the response variable)\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer, mean_squared_error\n",
    "\n",
    "#Function for Root mean squared error\n",
    "#https://stackoverflow.com/questions/17197492/root-mean-square-error-in-python\n",
    "def rmse(y_actual, y_predicted):\n",
    "    return np.sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "#Function for Mean Absolute Percentage Error (MAPE) - Untested\n",
    "#Adapted from - https://stackoverflow.com/questions/42250958/how-to-optimize-mape-code-in-python\n",
    "def mape(y_actual, y_predicted): \n",
    "    mask = y_actual != 0\n",
    "    return (np.fabs(y_actual - y_predicted)/y_actual)[mask].mean() * 100\n",
    "\n",
    "#Create scorers for rmse and mape functions\n",
    "mae_scorer = make_scorer(score_func=mean_absolute_error, greater_is_better=False)\n",
    "rmse_scorer = make_scorer(score_func=rmse, greater_is_better=False)\n",
    "mape_scorer = make_scorer(score_func=mape, greater_is_better=False)\n",
    "\n",
    "#Make scorer array to pass into cross_validate() function for producing mutiple scores for each cv fold.\n",
    "errorScoring = {'MAE':  mae_scorer, \n",
    "                'RMSE': rmse_scorer,\n",
    "                'MAPE': mape_scorer\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Regression Model Evaluation\n",
    "All regression models are evaluated using the regression model evaluation function below:\n",
    "\n",
    "The following regression evaluation function uses the cross validation object and the custom scorers in the two cells above in combination with sklearn.model_selection's cross_validate function to perform cross validation for regression estimators.\n",
    "The cross validation object above uses a random seed to ensure that all regression estimators are tested on the same randomly selected records for each cross validation fold.\n",
    "Custom scorers are created using the three chosen mean error scores and passed into cross_validate(), so all three scores are calcualted using a single call to cross_validate().\n",
    "All of this functionality is wrapped within the custom EvaluateRegressionEstimator() function below so multiple regression models may be tested using the same test / train cv data and evaluation scores producing a consistent output for each model without the need to re-write the same code over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def EvaluateRegressionEstimator(regEstimator, X, y, cv):\n",
    "    \n",
    "    scores = cross_validate(regEstimator, X, y, scoring=errorScoring, cv=cv, return_train_score=True)\n",
    "\n",
    "    #cross val score sign-flips the outputs of MAE\n",
    "    # https://github.com/scikit-learn/scikit-learn/issues/2439\n",
    "    scores['test_MAE'] = scores['test_MAE'] * -1\n",
    "    scores['test_MAPE'] = scores['test_MAPE'] * -1\n",
    "    scores['test_RMSE'] = scores['test_RMSE'] * -1\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    maeAvg = scores['test_MAE'].mean()\n",
    "    print_str = \"The average MAE for all cv folds is: \\t\\t\\t {maeAvg:.5}\"\n",
    "    print(print_str.format(maeAvg=maeAvg))\n",
    "\n",
    "    #print mean test_MAPE for all folds\n",
    "    scores['test_MAPE'] = scores['test_MAPE']\n",
    "    mape_avg = scores['test_MAPE'].mean()\n",
    "    print_str = \"The average MAE percentage (MAPE) for all cv folds is: \\t {mape_avg:.5}\"\n",
    "    print(print_str.format(mape_avg=mape_avg))\n",
    "\n",
    "    #print mean MAE for all folds \n",
    "    RMSEavg = scores['test_RMSE'].mean()\n",
    "    print_str = \"The average RMSE for all cv folds is: \\t\\t\\t {RMSEavg:.5}\"\n",
    "    print(print_str.format(RMSEavg=RMSEavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['MAE'] = scores['test_MAE']\n",
    "    scoresResults['MAPE'] = scores['test_MAPE']\n",
    "    scoresResults['RMSE'] = scores['test_RMSE']\n",
    "    return scoresResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses Support Vector Machines for regression of continuous variables (SVR). Please see documentation here:\"\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "http://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Linear regression object and perform a grid search to find the best parameters\n",
    "from sklearn.svm import SVR\n",
    "reg = SVR()\n",
    "\n",
    "#Set up SVR parameters to test (WARNING: Creates 320 models!!!) \n",
    "costs = [0.001, 0.1, 1, 10]\n",
    "defGamma = 1 / X.shape[1]  #This is the default value for the gamma parameter\n",
    "gammas = [defGamma, 0.1, 1, 10]\n",
    "kernels = ['rbf','linear']\n",
    "parameters = {'C': costs, 'gamma' : gammas, 'kernel': kernels}\n",
    "\n",
    "#Create a grid search object using the parameters above\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=reg\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "#Create a regression estimator with best parameters for cross validation\n",
    "regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "                   kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "EvaluateRegressionEstimator(regEstimator, X, Y, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do we predict graduation rates greater than 100%?\n",
    "regEstimator = SVR(C=0.001, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
    "                   kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "regEstimator.fit(X, Y)\n",
    "yhat = regEstimator.predict(X)\n",
    "print(\"Yhat Max: \", yhat.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses a custom Logit model based on transformations of the Linear Regression object. Please see documentation here:\"\n",
    "\n",
    "https://stackoverflow.com/questions/33845539/modelling-probabilities-in-a-regularized-logistic-regression-model-in-python\n",
    "https://stackoverflow.com/questions/44234682/how-to-use-sklearn-when-target-variable-is-a-proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class LogitRegression(LinearRegression):\n",
    "\n",
    "    def fit(self, x, p):\n",
    "        p[p==0] = 0.009    #0.1111111111111111 \n",
    "        p[p==1] = 0.991    #0.9999999999999999  big precision seems to kill MAE scores here?\n",
    "        #e = 0.0000000000000001\n",
    "        #p = p * e + 0.5 * e                    This technique was really bad too. \n",
    "        p = np.asarray(p)\n",
    "        y = np.log(p / (1 - p))\n",
    "        return super(LogitRegression, self).fit(x, y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        y = super(LogitRegression, self).predict(x)\n",
    "        yhat = 1 / (np.exp(-y) + 1)\n",
    "        yhat[yhat <= 0.009] = 0\n",
    "        yhat[yhat >= 0.991] = 1\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert y to a proability \n",
    "Y = Y / 100\n",
    "\n",
    "#Create a Linear Regression object and perform a grid search to find the best parameters\n",
    "linreg = LogitRegression()\n",
    "parameters = {'normalize':(True,False), 'fit_intercept':(True,False)}\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=linreg\n",
    "                   #, n_jobs=8 # jobs to run in parallel (This breaks the custom estimators for some reason!)\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring=mae_scorer)\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGridSearch.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the best estimator parameters\n",
    "regGridSearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CappedLinearRegression predictions between 0 and 100% using the best parameters for our Linear Regression object\n",
    "regEstimator = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics. \n",
    "EvaluateRegressionEstimator(regEstimator, X, Y, cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
