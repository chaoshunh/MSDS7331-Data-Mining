{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining - Mini Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "###### Data Description\n",
    "The Dallas Crime Incident data set that is used in the Mini Project acts as a bridge between the citizens of Dallas and the Dallas PD. It represents the Dallas Police Public Data - RMS Incidents from June 1, 2014 to September 7, 2018. \n",
    "For purposes of this Mini Project, the main dataframe that is trimmed based on the analysis performed as part of Lab 1. The details of the data quality clean up and choice of columns have been detailed in the Lab 1 notebook link provided below.\n",
    "Lab 1 Notebook Link - https://github.com/wtubin/MSDS7331-Data-Mining/MSDS7331_Data_Mining_Lab1_Data-Viz_Pre-Processing.ipynb\n",
    "\n",
    "\n",
    "#### Objective\n",
    "\n",
    "The objective of this unit is to perform Logistic Regression and Support Vector Machines categorization on the chosen data set and optimize the parameters in order to improve the accuracy of the model.\n",
    "GitHub Repository containing the artifacts - https://github.com/wtubin/MSDS7331-Data-Mining\n",
    "Location of the raw (compressed) data file - https://github.com/wtubin/MSDS7331-Data-Mining/Police_Incidents.7z \n",
    "\n",
    "The three models are:\n",
    "\n",
    "- Logistic Regression, using GridSearchCV, with manual variable reduction\n",
    "- Logistic Regression, using GridSearchCV, with Recursive Feature Elimination (RFE)\n",
    "- Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create Models\n",
    "\n",
    "##### Data Preparation\n",
    "\n",
    "The dataset is loaded and cleaning is performed with some modifications as needed to feed into model. The attributes with zero variance (closer) or those attributes that does not have value in predicting the respose variable are removed. For example attributes like beats, sector, Location1, etc. serves no purpose for our model. \n",
    "\n",
    "Then dataset will be splitted into explanatory, reffered as X (Attributes) and response referred as \"Y\" (response variable: \"Arrest_status\") for running the models.\n",
    "\n",
    "        - X : Explanatory variable (attributes)\n",
    "        - Y : Response variable (Res_time_category)\n",
    "\n",
    "The attributes will be scaled to have a mean of 0 and variance of 1 in order to imporve accuracy of the classification models. The data will then be splitted in to 80/20 training/test set split. To reduce possibility of \"overfitting\", 10-fold cross validation will be performed. The GrisdsearchCV method with manual variable reduction will be performed and we will be utilizing correlation scores, variance inflaion scores, variance inflation factors (VIFs) and significance for manual determination of attributes. This will help us reduce the attributes for our model.These remaining attributes will also be utilized in other two models: Logistic Regression using GridSearchCV with Recursive Feature Elimination and Support Vector Machine. The scikit-learn GridSearchCV feature will be utilized to adjust model parameters for adjusting class_weight.\n",
    "\n",
    "Overall, accuracy, precision, and recall is determined by utilizing the modification of original function created by Dr. Drew in his Education Data Notebook for clasification to check for our proper classification success. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255154, 44)\n",
      "(223803, 45)\n",
      "Year_of_Incident                0\n",
      "Service_Number_ID               0\n",
      "Watch                           0\n",
      "Type_of_Incident                0\n",
      "Type_Location                3828\n",
      "Reporting_Area                129\n",
      "Beat                           53\n",
      "Division                        0\n",
      "Sector                         53\n",
      "Council_District                0\n",
      "Day1_of_the_Week                0\n",
      "Call_Received_Date_Time         0\n",
      "Call_Cleared_Date_Time        148\n",
      "Call_Dispatch_Date_Time        21\n",
      "Person_Involvement_Type         0\n",
      "Victim_Type                     0\n",
      "Victim_Race                     0\n",
      "Victim_Gender                   0\n",
      "Victim_Age                      0\n",
      "Offense_Status                399\n",
      "Victim_Condition           206053\n",
      "Hate_Crime                      0\n",
      "Family_Offense                 29\n",
      "Weapon_Used                 23889\n",
      "Gang_Related_Offense            0\n",
      "Drug_Related                    0\n",
      "UCR_Offense_Name            12102\n",
      "RMS_Code                        0\n",
      "UCR_Code                    12102\n",
      "X_Coordinate                17588\n",
      "Y_Coordinate                17588\n",
      "Zip_Code                        0\n",
      "City                            0\n",
      "State                         536\n",
      "Location1                       0\n",
      "Call_Received                   0\n",
      "Call_Cleared                  148\n",
      "Call_Dispatch                  21\n",
      "Number_of_offense               0\n",
      "Response_time                   0\n",
      "Latitude                     8362\n",
      "Longitude                    8362\n",
      "Arrest_status                   0\n",
      "Call_Received_Hour              0\n",
      "Res_time_category               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path = \"../Data/\" # Generic path\n",
    "incident = pd.read_csv(path + 'LAB1_completed_Dataset_clean.csv', low_memory= False)\n",
    "print(incident.shape)\n",
    "incident['Response_time'] = incident['Response_time'].fillna(incident['Response_time'].mean()).astype(np.int)\n",
    "incident['Res_time_category'] = pd.cut(incident.Response_time,[0,20,1e6],2,labels=[0,1])\n",
    "incident = incident[incident['Res_time_category'].isnull()==False]\n",
    "# incident['Res_time_category'] = pd.Categorical(incident['Res_time_category']).codes\n",
    "\n",
    "incident = incident[incident['Call_Received_Hour'].isnull()==False]\n",
    "\n",
    "print(incident.shape)\n",
    "print(incident.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    142816\n",
      "1     80987\n",
      "Name: Res_time_category, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21c224166d8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(incident['Res_time_category'].value_counts())\n",
    "incident['Res_time_category'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCR_Offense_Name change this\n",
    "\n",
    "incident.loc[:,'UCR_Offense_Name'] = incident['UCR_Offense_Name'].fillna(\"MISSING\")\n",
    "\n",
    "THEFT_FRAUD     = dict.fromkeys(['THEFT/BMV', 'THEFT ORG RETAIL', 'BURGLARY-RESIDENCE', 'OTHER THEFTS',\n",
    "                                 'ROBBERY-INDIVIDUAL','THEFT/SHOPLIFT', 'BURGLARY-BUSINESS', 'FORGE & COUNTERFEIT', \n",
    "                                 'FRAUD', 'EMBEZZLEMENT','ROBBERY-BUSINESS','THEFT ORG RETAIL'],\"THEFT_FRAUD\" ) \n",
    "MVA_TRAFFIC      =dict.fromkeys(['ACCIDENT MV', 'MOTOR VEHICLE ACCIDENT', 'UUMV', 'TRAFFIC VIOLATION',\n",
    "                                 'TRAFFIC FATALITY'],\"MVA_TRAFFIC\" )        \n",
    "WEAPONS_FIREARMS =dict.fromkeys(['WEAPONS', 'ARSON', 'INJURED FIREARM'], \"WEAPONS_FIREARMS\")         \n",
    "ASSUALT          = dict.fromkeys(['ASSAULT','VANDALISM & CRIM MISCHIEF', 'AGG ASSAULT - NFV', 'OFFENSE AGAINST CHILD',\n",
    "                                  'AGG ASSAULT - FV'], \"ASSUALT\")\n",
    "OTHERS_THREATS   = dict.fromkeys(['FOUND', 'OTHERS', 'LOST', 'CRIMINAL TRESPASS', 'DISORDERLY CONDUCT', \n",
    "                                  'ANIMAL BITE','INJURED HOME','INJURED PUBLIC', 'TERRORISTIC THREAT', \n",
    "                                  'EVADING', 'INJURED OCCUPA', 'ORANIZED CRIME', 'KIDNAPPING', \n",
    "                                  'RESIST ARREST','FAIL TO ID', 'HUMAN TRAFFICKING', 'MISSING'], \"OTHERS_THREATS\")\n",
    "INTOXICATION     = dict.fromkeys(['DRUNK & DISORDERLY', 'DWI', 'NARCOTICS & DRUGS', 'LIQUOR OFFENSE', \n",
    "                                  'INTOXICATION MANSLAUGHTER'],\"INTOXICATION\")\n",
    "MURDER_DEATH     = dict.fromkeys(['SUDDEN DEATH&FOUND BODIES','MURDER'], \"MURDER_DEATH\")\n",
    "                    \n",
    "\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(THEFT_FRAUD)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(MVA_TRAFFIC)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(WEAPONS_FIREARMS)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(ASSUALT)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(OTHERS_THREATS)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(INTOXICATION)\n",
    "incident.loc[:,'UCR_Offense_Name']= incident['UCR_Offense_Name'].replace(MURDER_DEATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FILTERING OUT UNNECESSARY NULL DATA\n",
    "incident = incident[incident['Watch']!=0]\n",
    "incident = incident[(incident['Victim_Age']>=0) & (incident['Victim_Age']<=90)]\n",
    "incident = incident[incident['Victim_Race']!=\"Unknown\"]\n",
    "incident = incident[incident['Victim_Type']!=\"Unknown\"]\n",
    "incident= incident[incident.Number_of_offense != \"RP\"]\n",
    "\n",
    "incident = incident[incident['Victim_Gender']!=\"U\"]\n",
    "incident.loc[:,'IsMale'] = incident.Victim_Gender=='M' \n",
    "incident.IsMale = incident.IsMale.astype(np.int)\n",
    "\n",
    "incident.loc[:,'Social_crime_score'] = incident['Hate_Crime']+incident['Gang_Related_Offense']+incident['Drug_Related']\n",
    "\n",
    "incident.loc[:,'Victim_Age'] = incident['Victim_Age'].astype(np.int)\n",
    "incident.loc[:,'Victim_Age_Group'] = pd.cut(incident.Victim_Age,[-1,18,30,60,999],4,labels=[0,1,2,3])\n",
    "\n",
    "# incident['UCR_Offense_Name'] = pd.Categorical(incident['UCR_Offense_Name']).codes\n",
    "incident['Day1_of_the_Week'] = pd.Categorical(incident['Day1_of_the_Week']).codes\n",
    "# incident['Division'] = pd.Categorical(incident['Division']).codes\n",
    "incident['Victim_Type'] = pd.Categorical(incident['Victim_Type']).codes\n",
    "incident['Victim_Race'] = pd.Categorical(incident['Victim_Race']).codes\n",
    "\n",
    "incident['Number_of_offense']= incident.Number_of_offense.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fist 2 characters in the RMS_Code is offense degree.\n",
    "\n",
    "incident['Degree']=incident['RMS_Code'].astype(str).str[:2]\n",
    "incident['Degree_Fact'] = pd.Categorical(incident['Degree']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.get_dummies(incident.Division,prefix='Div')\n",
    "incident = pd.concat((incident,tmp_df),axis=1)\n",
    "\n",
    "tmp_df = pd.get_dummies(incident.UCR_Offense_Name,prefix='UCR_')\n",
    "incident = pd.concat((incident,tmp_df),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year_of_Incident', 'Service_Number_ID', 'Watch', 'Type_of_Incident',\n",
       "       'Type_Location', 'Reporting_Area', 'Beat', 'Division', 'Sector',\n",
       "       'Council_District', 'Day1_of_the_Week', 'Call_Received_Date_Time',\n",
       "       'Call_Cleared_Date_Time', 'Call_Dispatch_Date_Time',\n",
       "       'Person_Involvement_Type', 'Victim_Type', 'Victim_Race',\n",
       "       'Victim_Gender', 'Victim_Age', 'Offense_Status', 'Victim_Condition',\n",
       "       'Hate_Crime', 'Family_Offense', 'Weapon_Used', 'Gang_Related_Offense',\n",
       "       'Drug_Related', 'UCR_Offense_Name', 'RMS_Code', 'UCR_Code',\n",
       "       'X_Coordinate', 'Y_Coordinate', 'Zip_Code', 'City', 'State',\n",
       "       'Location1', 'Call_Received', 'Call_Cleared', 'Call_Dispatch',\n",
       "       'Number_of_offense', 'Response_time', 'Latitude', 'Longitude',\n",
       "       'Arrest_status', 'Call_Received_Hour', 'Res_time_category', 'IsMale',\n",
       "       'Social_crime_score', 'Victim_Age_Group', 'Degree', 'Degree_Fact',\n",
       "       'Div_CENTRAL', 'Div_NORTH CENTRAL', 'Div_NORTHEAST', 'Div_NORTHWEST',\n",
       "       'Div_SOUTH CENTRAL', 'Div_SOUTHEAST', 'Div_SOUTHWEST', 'Div_Unknown',\n",
       "       'UCR__ASSUALT', 'UCR__INTOXICATION', 'UCR__MURDER_DEATH',\n",
       "       'UCR__MVA_TRAFFIC', 'UCR__OTHERS_THREATS', 'UCR__THEFT_FRAUD',\n",
       "       'UCR__WEAPONS_FIREARMS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incident.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y Response variable dataframe\n",
    "inci_Y = incident['Res_time_category']\n",
    "\n",
    "# Attributes with no predictive features with respect to resposne variable\n",
    "\n",
    "# incident = incident.drop(['Year_of_Incident','Service_Number_ID','Type_of_Incident','Type_Location', 'Reporting_Area', \n",
    "#                           'Beat', 'Division', 'Sector', 'Council_District', 'Call_Received_Date_Time', \n",
    "#                           'Call_Cleared_Date_Time', 'Call_Dispatch_Date_Time','Person_Involvement_Type', 'Offense_Status',\n",
    "#                           'Victim_Condition','Family_Offense', 'Weapon_Used', 'RMS_Code', 'UCR_Code', \n",
    "#                           'Zip_Code', 'City', 'State','Location1', 'Call_Received', 'Call_Cleared', 'X_Coordinate', \n",
    "#                           'Y_Coordinate','Call_Dispatch', 'Latitude', 'Longitude','Victim_Gender',\n",
    "#                           'Res_time_category','Victim_Age_Group','Response_time', 'Degree', 'Number_of_offense', \n",
    "#                           'Watch'],axis=1)\n",
    "\n",
    "\n",
    "incident = incident.drop(['Year_of_Incident','Service_Number_ID','Type_of_Incident','Type_Location', 'Reporting_Area', \n",
    "                          'Beat','Sector', 'Council_District', 'Call_Received_Date_Time', \n",
    "                          'Call_Cleared_Date_Time', 'Call_Dispatch_Date_Time','Person_Involvement_Type', 'Offense_Status',\n",
    "                          'Victim_Condition','Family_Offense', 'Weapon_Used', 'RMS_Code', 'UCR_Code', \n",
    "                          'Zip_Code', 'City', 'State','Location1', 'Call_Received', 'Call_Cleared', 'X_Coordinate', \n",
    "                          'Y_Coordinate','Call_Dispatch', 'Latitude', 'Longitude','Victim_Gender', \n",
    "                          'Response_time', 'Degree', 'Watch','Victim_Age_Group', 'Res_time_category',\n",
    "                         'Victim_Type','Victim_Race','Victim_Age', 'Gang_Related_Offense','Hate_Crime',\n",
    "                            'Drug_Related', 'Division', 'UCR_Offense_Name'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINS MODEL\n",
    "# ['Day1_of_the_Week',  'Division','Responsetime_cat','Arrest_status','Social_crime_score', \n",
    "#  'IsMale','Call_Received_Hour','UCR_Offense_Name','Degree_Fact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day1_of_the_Week</th>\n",
       "      <th>Number_of_offense</th>\n",
       "      <th>Arrest_status</th>\n",
       "      <th>Call_Received_Hour</th>\n",
       "      <th>IsMale</th>\n",
       "      <th>Social_crime_score</th>\n",
       "      <th>Degree_Fact</th>\n",
       "      <th>Div_CENTRAL</th>\n",
       "      <th>Div_NORTH CENTRAL</th>\n",
       "      <th>Div_NORTHEAST</th>\n",
       "      <th>...</th>\n",
       "      <th>Div_SOUTHEAST</th>\n",
       "      <th>Div_SOUTHWEST</th>\n",
       "      <th>Div_Unknown</th>\n",
       "      <th>UCR__ASSUALT</th>\n",
       "      <th>UCR__INTOXICATION</th>\n",
       "      <th>UCR__MURDER_DEATH</th>\n",
       "      <th>UCR__MVA_TRAFFIC</th>\n",
       "      <th>UCR__OTHERS_THREATS</th>\n",
       "      <th>UCR__THEFT_FRAUD</th>\n",
       "      <th>UCR__WEAPONS_FIREARMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82790</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209090</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Day1_of_the_Week  Number_of_offense  Arrest_status  \\\n",
       "82790                  4                  1              0   \n",
       "209090                 2                  1              0   \n",
       "\n",
       "        Call_Received_Hour  IsMale  Social_crime_score  Degree_Fact  \\\n",
       "82790                 19.0       0                   0            8   \n",
       "209090                20.0       0                   0            1   \n",
       "\n",
       "        Div_CENTRAL  Div_NORTH CENTRAL  Div_NORTHEAST          ...            \\\n",
       "82790             0                  0              0          ...             \n",
       "209090            1                  0              0          ...             \n",
       "\n",
       "        Div_SOUTHEAST  Div_SOUTHWEST  Div_Unknown  UCR__ASSUALT  \\\n",
       "82790               0              0            0             1   \n",
       "209090              0              0            0             0   \n",
       "\n",
       "        UCR__INTOXICATION  UCR__MURDER_DEATH  UCR__MVA_TRAFFIC  \\\n",
       "82790                   0                  0                 0   \n",
       "209090                  0                  0                 0   \n",
       "\n",
       "        UCR__OTHERS_THREATS  UCR__THEFT_FRAUD  UCR__WEAPONS_FIREARMS  \n",
       "82790                     0                 0                      0  \n",
       "209090                    0                 1                      0  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incident.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day1_of_the_Week</th>\n",
       "      <th>Number_of_offense</th>\n",
       "      <th>Arrest_status</th>\n",
       "      <th>Call_Received_Hour</th>\n",
       "      <th>IsMale</th>\n",
       "      <th>Social_crime_score</th>\n",
       "      <th>Degree_Fact</th>\n",
       "      <th>Div_CENTRAL</th>\n",
       "      <th>Div_NORTH CENTRAL</th>\n",
       "      <th>Div_NORTHEAST</th>\n",
       "      <th>...</th>\n",
       "      <th>Div_SOUTHEAST</th>\n",
       "      <th>Div_SOUTHWEST</th>\n",
       "      <th>Div_Unknown</th>\n",
       "      <th>UCR__ASSUALT</th>\n",
       "      <th>UCR__INTOXICATION</th>\n",
       "      <th>UCR__MURDER_DEATH</th>\n",
       "      <th>UCR__MVA_TRAFFIC</th>\n",
       "      <th>UCR__OTHERS_THREATS</th>\n",
       "      <th>UCR__THEFT_FRAUD</th>\n",
       "      <th>UCR__WEAPONS_FIREARMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.00000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.00000</td>\n",
       "      <td>200780.000000</td>\n",
       "      <td>200780.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.958756</td>\n",
       "      <td>1.087623</td>\n",
       "      <td>0.099118</td>\n",
       "      <td>12.676317</td>\n",
       "      <td>0.531911</td>\n",
       "      <td>0.012740</td>\n",
       "      <td>6.503586</td>\n",
       "      <td>0.125082</td>\n",
       "      <td>0.110868</td>\n",
       "      <td>0.179993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166446</td>\n",
       "      <td>0.153257</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.27943</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.014997</td>\n",
       "      <td>0.146160</td>\n",
       "      <td>0.13979</td>\n",
       "      <td>0.416585</td>\n",
       "      <td>0.001629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.996215</td>\n",
       "      <td>0.416750</td>\n",
       "      <td>0.298822</td>\n",
       "      <td>6.270955</td>\n",
       "      <td>0.498982</td>\n",
       "      <td>0.113783</td>\n",
       "      <td>2.773755</td>\n",
       "      <td>0.330813</td>\n",
       "      <td>0.313969</td>\n",
       "      <td>0.384183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372481</td>\n",
       "      <td>0.360236</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.44872</td>\n",
       "      <td>0.037517</td>\n",
       "      <td>0.121539</td>\n",
       "      <td>0.353267</td>\n",
       "      <td>0.34677</td>\n",
       "      <td>0.492994</td>\n",
       "      <td>0.040324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Day1_of_the_Week  Number_of_offense  Arrest_status  Call_Received_Hour  \\\n",
       "count     200780.000000      200780.000000  200780.000000       200780.000000   \n",
       "mean           2.958756           1.087623       0.099118           12.676317   \n",
       "std            1.996215           0.416750       0.298822            6.270955   \n",
       "min            0.000000           1.000000       0.000000            0.000000   \n",
       "25%            1.000000           1.000000       0.000000            8.000000   \n",
       "50%            3.000000           1.000000       0.000000           13.000000   \n",
       "75%            5.000000           1.000000       0.000000           18.000000   \n",
       "max            6.000000          22.000000       1.000000           23.000000   \n",
       "\n",
       "              IsMale  Social_crime_score    Degree_Fact    Div_CENTRAL  \\\n",
       "count  200780.000000       200780.000000  200780.000000  200780.000000   \n",
       "mean        0.531911            0.012740       6.503586       0.125082   \n",
       "std         0.498982            0.113783       2.773755       0.330813   \n",
       "min         0.000000            0.000000       0.000000       0.000000   \n",
       "25%         0.000000            0.000000       4.000000       0.000000   \n",
       "50%         1.000000            0.000000       7.000000       0.000000   \n",
       "75%         1.000000            0.000000       8.000000       0.000000   \n",
       "max         1.000000            2.000000      11.000000       1.000000   \n",
       "\n",
       "       Div_NORTH CENTRAL  Div_NORTHEAST          ...            Div_SOUTHEAST  \\\n",
       "count      200780.000000  200780.000000          ...            200780.000000   \n",
       "mean            0.110868       0.179993          ...                 0.166446   \n",
       "std             0.313969       0.384183          ...                 0.372481   \n",
       "min             0.000000       0.000000          ...                 0.000000   \n",
       "25%             0.000000       0.000000          ...                 0.000000   \n",
       "50%             0.000000       0.000000          ...                 0.000000   \n",
       "75%             0.000000       0.000000          ...                 0.000000   \n",
       "max             1.000000       1.000000          ...                 1.000000   \n",
       "\n",
       "       Div_SOUTHWEST    Div_Unknown  UCR__ASSUALT  UCR__INTOXICATION  \\\n",
       "count  200780.000000  200780.000000  200780.00000      200780.000000   \n",
       "mean        0.153257       0.000254       0.27943           0.001410   \n",
       "std         0.360236       0.015936       0.44872           0.037517   \n",
       "min         0.000000       0.000000       0.00000           0.000000   \n",
       "25%         0.000000       0.000000       0.00000           0.000000   \n",
       "50%         0.000000       0.000000       0.00000           0.000000   \n",
       "75%         0.000000       0.000000       1.00000           0.000000   \n",
       "max         1.000000       1.000000       1.00000           1.000000   \n",
       "\n",
       "       UCR__MURDER_DEATH  UCR__MVA_TRAFFIC  UCR__OTHERS_THREATS  \\\n",
       "count      200780.000000     200780.000000         200780.00000   \n",
       "mean            0.014997          0.146160              0.13979   \n",
       "std             0.121539          0.353267              0.34677   \n",
       "min             0.000000          0.000000              0.00000   \n",
       "25%             0.000000          0.000000              0.00000   \n",
       "50%             0.000000          0.000000              0.00000   \n",
       "75%             0.000000          0.000000              0.00000   \n",
       "max             1.000000          1.000000              1.00000   \n",
       "\n",
       "       UCR__THEFT_FRAUD  UCR__WEAPONS_FIREARMS  \n",
       "count     200780.000000          200780.000000  \n",
       "mean           0.416585               0.001629  \n",
       "std            0.492994               0.040324  \n",
       "min            0.000000               0.000000  \n",
       "25%            0.000000               0.000000  \n",
       "50%            0.000000               0.000000  \n",
       "75%            1.000000               0.000000  \n",
       "max            1.000000               1.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incident.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation\n",
    "\n",
    "    1. The data is divided into 80/20 train -test split.\n",
    "    2. 10 folds cross validation\n",
    "    3. Random seed with random state 0 for random test and training splits for each iteration of cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200780 entries, 0 to 255153\n",
      "Data columns (total 22 columns):\n",
      "Day1_of_the_Week         200780 non-null int8\n",
      "Number_of_offense        200780 non-null int32\n",
      "Arrest_status            200780 non-null int64\n",
      "Call_Received_Hour       200780 non-null float64\n",
      "IsMale                   200780 non-null int32\n",
      "Social_crime_score       200780 non-null int64\n",
      "Degree_Fact              200780 non-null int8\n",
      "Div_CENTRAL              200780 non-null uint8\n",
      "Div_NORTH CENTRAL        200780 non-null uint8\n",
      "Div_NORTHEAST            200780 non-null uint8\n",
      "Div_NORTHWEST            200780 non-null uint8\n",
      "Div_SOUTH CENTRAL        200780 non-null uint8\n",
      "Div_SOUTHEAST            200780 non-null uint8\n",
      "Div_SOUTHWEST            200780 non-null uint8\n",
      "Div_Unknown              200780 non-null uint8\n",
      "UCR__ASSUALT             200780 non-null uint8\n",
      "UCR__INTOXICATION        200780 non-null uint8\n",
      "UCR__MURDER_DEATH        200780 non-null uint8\n",
      "UCR__MVA_TRAFFIC         200780 non-null uint8\n",
      "UCR__OTHERS_THREATS      200780 non-null uint8\n",
      "UCR__THEFT_FRAUD         200780 non-null uint8\n",
      "UCR__WEAPONS_FIREARMS    200780 non-null uint8\n",
      "dtypes: float64(1), int32(2), int64(2), int8(2), uint8(15)\n",
      "memory usage: 10.9 MB\n",
      "inci_X None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200780 entries, 0 to 255153\n",
      "Data columns (total 22 columns):\n",
      "Day1_of_the_Week         200780 non-null int8\n",
      "Number_of_offense        200780 non-null int32\n",
      "Arrest_status            200780 non-null int64\n",
      "Call_Received_Hour       200780 non-null float64\n",
      "IsMale                   200780 non-null int32\n",
      "Social_crime_score       200780 non-null int64\n",
      "Degree_Fact              200780 non-null int8\n",
      "Div_CENTRAL              200780 non-null uint8\n",
      "Div_NORTH CENTRAL        200780 non-null uint8\n",
      "Div_NORTHEAST            200780 non-null uint8\n",
      "Div_NORTHWEST            200780 non-null uint8\n",
      "Div_SOUTH CENTRAL        200780 non-null uint8\n",
      "Div_SOUTHEAST            200780 non-null uint8\n",
      "Div_SOUTHWEST            200780 non-null uint8\n",
      "Div_Unknown              200780 non-null uint8\n",
      "UCR__ASSUALT             200780 non-null uint8\n",
      "UCR__INTOXICATION        200780 non-null uint8\n",
      "UCR__MURDER_DEATH        200780 non-null uint8\n",
      "UCR__MVA_TRAFFIC         200780 non-null uint8\n",
      "UCR__OTHERS_THREATS      200780 non-null uint8\n",
      "UCR__THEFT_FRAUD         200780 non-null uint8\n",
      "UCR__WEAPONS_FIREARMS    200780 non-null uint8\n",
      "dtypes: float64(1), int32(2), int64(2), int8(2), uint8(15)\n",
      "memory usage: 10.9 MB\n",
      "inci_X_Rfe None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 200780 entries, 0 to 255153\n",
      "Data columns (total 22 columns):\n",
      "Day1_of_the_Week         200780 non-null int8\n",
      "Number_of_offense        200780 non-null int32\n",
      "Arrest_status            200780 non-null int64\n",
      "Call_Received_Hour       200780 non-null float64\n",
      "IsMale                   200780 non-null int32\n",
      "Social_crime_score       200780 non-null int64\n",
      "Degree_Fact              200780 non-null int8\n",
      "Div_CENTRAL              200780 non-null uint8\n",
      "Div_NORTH CENTRAL        200780 non-null uint8\n",
      "Div_NORTHEAST            200780 non-null uint8\n",
      "Div_NORTHWEST            200780 non-null uint8\n",
      "Div_SOUTH CENTRAL        200780 non-null uint8\n",
      "Div_SOUTHEAST            200780 non-null uint8\n",
      "Div_SOUTHWEST            200780 non-null uint8\n",
      "Div_Unknown              200780 non-null uint8\n",
      "UCR__ASSUALT             200780 non-null uint8\n",
      "UCR__INTOXICATION        200780 non-null uint8\n",
      "UCR__MURDER_DEATH        200780 non-null uint8\n",
      "UCR__MVA_TRAFFIC         200780 non-null uint8\n",
      "UCR__OTHERS_THREATS      200780 non-null uint8\n",
      "UCR__THEFT_FRAUD         200780 non-null uint8\n",
      "UCR__WEAPONS_FIREARMS    200780 non-null uint8\n",
      "dtypes: float64(1), int32(2), int64(2), int8(2), uint8(15)\n",
      "memory usage: 10.9 MB\n",
      "inci_X_SVM None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "#Create Cross Validation Object with 10 folds with 80/20 train - test split\n",
    "cv = ShuffleSplit(n_splits = 10, test_size=0.20, random_state=0)\n",
    "\n",
    "#Create X Explanatory Variables DF to support the individual models\n",
    "inci_X = incident\n",
    "\n",
    "inci_X_Rfe = incident\n",
    "inci_X_SVM = incident\n",
    "print(\"inci_X\", inci_X.info())\n",
    "print(\"inci_X_Rfe\", inci_X_Rfe.info())\n",
    "print(\"inci_X_SVM\", inci_X_SVM.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Colinearity\n",
    "\n",
    "The dataset had few issues with collinearity, this issue was found during ther LAB 1 iteration of data exploration. \n",
    "\n",
    "###### Starting Colinearity\n",
    "Some of the attributes which were hightly correlated comes from either creation of new columns or they are derived from date related varibales or splitted from original categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCR__OTHERS_THREATS  Degree_Fact            0.210516\n",
      "Arrest_status        UCR__ASSUALT           0.206600\n",
      "Degree_Fact          UCR__ASSUALT           0.159513\n",
      "UCR__MVA_TRAFFIC     Degree_Fact            0.152628\n",
      "UCR__MURDER_DEATH    Degree_Fact            0.113504\n",
      "                                              ...   \n",
      "UCR__MVA_TRAFFIC     UCR__ASSUALT          -0.257647\n",
      "UCR__THEFT_FRAUD     UCR__OTHERS_THREATS   -0.340642\n",
      "                     UCR__MVA_TRAFFIC      -0.349614\n",
      "Degree_Fact          UCR__THEFT_FRAUD      -0.433983\n",
      "UCR__THEFT_FRAUD     UCR__ASSUALT          -0.526213\n",
      "Length: 231, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Create correlation matrix\n",
    "CorrMat = incident.corr()\n",
    "\n",
    "# Highest Correlation Pairs\n",
    "corrPairs = CorrMat.unstack().sort_values(kind=\"quicksort\", ascending=False)\n",
    "#- REMOVE DUPLICATES\n",
    "corrPairs = corrPairs[::2]\n",
    "corrPairs = corrPairs[corrPairs.index.get_level_values(0) != corrPairs.index.get_level_values(1)]\n",
    "with pd.option_context('display.max_rows',10):\n",
    "    print(corrPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ending Colinearity\n",
    "The highly correlated attributes were manually removed from dataset. \n",
    "- Total attributes removed : _____****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCR__OTHERS_THREATS  Degree_Fact            0.210516\n",
      "Arrest_status        UCR__ASSUALT           0.206600\n",
      "Degree_Fact          UCR__ASSUALT           0.159513\n",
      "UCR__MVA_TRAFFIC     Degree_Fact            0.152628\n",
      "UCR__MURDER_DEATH    Degree_Fact            0.113504\n",
      "                                              ...   \n",
      "UCR__MVA_TRAFFIC     UCR__ASSUALT          -0.257647\n",
      "UCR__THEFT_FRAUD     UCR__OTHERS_THREATS   -0.340642\n",
      "                     UCR__MVA_TRAFFIC      -0.349614\n",
      "Degree_Fact          UCR__THEFT_FRAUD      -0.433983\n",
      "UCR__THEFT_FRAUD     UCR__ASSUALT          -0.526213\n",
      "Length: 231, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Drop highly correlated, insignificant and high VIF columns.\n",
    "# inci_X = incident.drop(['Drug_Related', 'Call_Received_Hour', 'Gang_Related_Offense'], axis=1)\n",
    "\n",
    "#Create correlation matrix\n",
    "CorrMat = inci_X.corr()\n",
    "\n",
    "# Highest Correlation Pairs\n",
    "corrPairs = CorrMat.unstack().sort_values(kind=\"quicksort\", ascending=False)\n",
    "#- REMOVE DUPLICATES\n",
    "corrPairs = corrPairs[::2]\n",
    "corrPairs = corrPairs[corrPairs.index.get_level_values(0) != corrPairs.index.get_level_values(1)]\n",
    "with pd.option_context('display.max_rows',10):\n",
    "    print(corrPairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scale Data\n",
    "In order to imporve accuracy and performance of our classification model and to prevent emphasis of one attribute over the other, attributes are scaled to have a mean of 0 and variance of 1 for all models in this report.Several features in the data set are decimal measurements that will never exceed 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Scale data\n",
    "scaler = StandardScaler()\n",
    "inci_X_scaled = scaler.fit_transform(inci_X)\n",
    "inci_X_Rfe_scaled = scaler.fit_transform(inci_X_Rfe)\n",
    "inci_X_SVM_scaled = scaler.fit_transform(inci_X_SVM)\n",
    "\n",
    "#Save as data frames\n",
    "df_inci_X_scaled = pd.DataFrame(inci_X_scaled)\n",
    "df_inci_X_Rfe_scaled = pd.DataFrame(inci_X_Rfe_scaled)\n",
    "df_inci_X_SVM_scaled= pd.DataFrame(inci_X_SVM_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Inflation Factors (VIF)\n",
    "\n",
    "The attributes analysis and scaling is indicated with hight variance inflation factors. Generally acceptable value should be under 10. This will help create better model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initially, for manual reduction method for Logistic regression VIF is, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001417</td>\n",
       "      <td>Day1_of_the_Week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.020348</td>\n",
       "      <td>Number_of_offense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.067255</td>\n",
       "      <td>Arrest_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.006317</td>\n",
       "      <td>Call_Received_Hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.017149</td>\n",
       "      <td>IsMale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.020684</td>\n",
       "      <td>Social_crime_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.294221</td>\n",
       "      <td>Degree_Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_NORTH CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_NORTHEAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_NORTHWEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_SOUTH CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_SOUTHEAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_SOUTHWEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__ASSUALT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__INTOXICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__MURDER_DEATH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__MVA_TRAFFIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__OTHERS_THREATS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__THEFT_FRAUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__WEAPONS_FIREARMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    VIF Factor               features\n",
       "0     1.001417       Day1_of_the_Week\n",
       "1     1.020348      Number_of_offense\n",
       "2     1.067255          Arrest_status\n",
       "3     1.006317     Call_Received_Hour\n",
       "4     1.017149                 IsMale\n",
       "5     1.020684     Social_crime_score\n",
       "6     1.294221            Degree_Fact\n",
       "7          inf            Div_CENTRAL\n",
       "8          inf      Div_NORTH CENTRAL\n",
       "9          inf          Div_NORTHEAST\n",
       "10         inf          Div_NORTHWEST\n",
       "11         inf      Div_SOUTH CENTRAL\n",
       "12         inf          Div_SOUTHEAST\n",
       "13         inf          Div_SOUTHWEST\n",
       "14         inf            Div_Unknown\n",
       "15         inf           UCR__ASSUALT\n",
       "16         inf      UCR__INTOXICATION\n",
       "17         inf      UCR__MURDER_DEATH\n",
       "18         inf       UCR__MVA_TRAFFIC\n",
       "19         inf    UCR__OTHERS_THREATS\n",
       "20         inf       UCR__THEFT_FRAUD\n",
       "21         inf  UCR__WEAPONS_FIREARMS"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "df2_vif = pd.DataFrame()\n",
    "df2_vif[\"VIF Factor\"] = [vif(inci_X.values, i) for i in range(inci_X.shape[1])]\n",
    "df2_vif[\"features\"] = inci_X.columns\n",
    "df2_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying threshold of 10 VIF is,\n",
    "After applying a threshold of 10 and using the Logistic Regression-with manual variable reduction, dataset, the VIF factors have been reduced significantly and are in an acceptable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001383e+00</td>\n",
       "      <td>Day1_of_the_Week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.020348e+00</td>\n",
       "      <td>Number_of_offense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.067249e+00</td>\n",
       "      <td>Arrest_status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.006317e+00</td>\n",
       "      <td>Call_Received_Hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.017149e+00</td>\n",
       "      <td>IsMale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.020684e+00</td>\n",
       "      <td>Social_crime_score</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.294149e+00</td>\n",
       "      <td>Degree_Fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.439147e+05</td>\n",
       "      <td>Div_NORTH CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_NORTHEAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_NORTHWEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_SOUTH CENTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_SOUTHEAST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_SOUTHWEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>inf</td>\n",
       "      <td>Div_Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__ASSUALT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__INTOXICATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__MURDER_DEATH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__MVA_TRAFFIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__OTHERS_THREATS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__THEFT_FRAUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>inf</td>\n",
       "      <td>UCR__WEAPONS_FIREARMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      VIF Factor               features\n",
       "0   1.001383e+00       Day1_of_the_Week\n",
       "1   1.020348e+00      Number_of_offense\n",
       "2   1.067249e+00          Arrest_status\n",
       "3   1.006317e+00     Call_Received_Hour\n",
       "4   1.017149e+00                 IsMale\n",
       "5   1.020684e+00     Social_crime_score\n",
       "6   1.294149e+00            Degree_Fact\n",
       "7            inf            Div_CENTRAL\n",
       "8   8.439147e+05      Div_NORTH CENTRAL\n",
       "9            inf          Div_NORTHEAST\n",
       "10           inf          Div_NORTHWEST\n",
       "11           inf      Div_SOUTH CENTRAL\n",
       "12           inf          Div_SOUTHEAST\n",
       "13           inf          Div_SOUTHWEST\n",
       "14           inf            Div_Unknown\n",
       "15           inf           UCR__ASSUALT\n",
       "16           inf      UCR__INTOXICATION\n",
       "17           inf      UCR__MURDER_DEATH\n",
       "18           inf       UCR__MVA_TRAFFIC\n",
       "19           inf    UCR__OTHERS_THREATS\n",
       "20           inf       UCR__THEFT_FRAUD\n",
       "21           inf  UCR__WEAPONS_FIREARMS"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Credit to:\n",
    "###https://stats.stackexchange.com/questions/155028/how-to-systematically-remove-collinear-variables-in-python\n",
    "###https://etav.github.io/python/vif_factor_python.html\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n",
    "\n",
    "df2_vif = pd.DataFrame()\n",
    "df2_vif[\"VIF Factor\"] = [vif(df_inci_X_scaled.values, i) for i in range(df_inci_X_scaled.shape[1])]\n",
    "df2_vif[\"features\"] = inci_X.columns\n",
    "df2_vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE SIGNIFICANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.632328\n",
      "         Iterations 6\n",
      "                            Results: Logit\n",
      "======================================================================\n",
      "Model:               Logit              Pseudo R-squared:  0.025      \n",
      "Dependent Variable:  Res_time_category  AIC:               253959.7509\n",
      "Date:                2018-10-03 12:26   BIC:               254174.1602\n",
      "No. Observations:    200780             Log-Likelihood:    -1.2696e+05\n",
      "Df Model:            20                 LL-Null:           -1.3026e+05\n",
      "Df Residuals:        200759             LLR p-value:       0.0000     \n",
      "Converged:           1.0000             Scale:             1.0000     \n",
      "No. Iterations:      6.0000                                           \n",
      "----------------------------------------------------------------------\n",
      "                       Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "----------------------------------------------------------------------\n",
      "Day1_of_the_Week      -0.0041   0.0024  -1.7305 0.0835 -0.0088  0.0005\n",
      "Number_of_offense     -0.2177   0.0142 -15.2957 0.0000 -0.2456 -0.1898\n",
      "Arrest_status         -0.6009   0.0184 -32.7231 0.0000 -0.6369 -0.5649\n",
      "Call_Received_Hour     0.0283   0.0008  36.4157 0.0000  0.0267  0.0298\n",
      "IsMale                -0.0616   0.0096  -6.4281 0.0000 -0.0804 -0.0428\n",
      "Social_crime_score    -0.7519   0.0547 -13.7569 0.0000 -0.8590 -0.6448\n",
      "Degree_Fact            0.0486   0.0020  24.1795 0.0000  0.0446  0.0525\n",
      "Div_CENTRAL           -0.5607      nan      nan    nan     nan     nan\n",
      "Div_NORTH CENTRAL     -0.7794      nan      nan    nan     nan     nan\n",
      "Div_NORTHEAST         -0.6233      nan      nan    nan     nan     nan\n",
      "Div_NORTHWEST         -0.4860      nan      nan    nan     nan     nan\n",
      "Div_SOUTH CENTRAL     -0.6892      nan      nan    nan     nan     nan\n",
      "Div_SOUTHEAST         -0.7734      nan      nan    nan     nan     nan\n",
      "Div_SOUTHWEST         -0.6486      nan      nan    nan     nan     nan\n",
      "Div_Unknown           -0.3353      nan      nan    nan     nan     nan\n",
      "UCR__ASSUALT          -0.4559      nan      nan    nan     nan     nan\n",
      "UCR__INTOXICATION     -1.1521      nan      nan    nan     nan     nan\n",
      "UCR__MURDER_DEATH     -1.1430      nan      nan    nan     nan     nan\n",
      "UCR__MVA_TRAFFIC      -0.1911      nan      nan    nan     nan     nan\n",
      "UCR__OTHERS_THREATS   -0.7040      nan      nan    nan     nan     nan\n",
      "UCR__THEFT_FRAUD      -0.0784      nan      nan    nan     nan     nan\n",
      "UCR__WEAPONS_FIREARMS -1.1714      nan      nan    nan     nan     nan\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION: SUMMARY TABLE WITHOUT SCALING- FEATURE SIGNIFICANCE, CROSS VALIDATION OF FULL MODEL\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "logit_model = sm.Logit(inci_Y, inci_X)\n",
    "result = logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresssion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifier Evaluation\n",
    "\n",
    "- Functions and code utilized from Dr. Drew's NC models \n",
    "https://github.com/jakemdrew/EducationDataNC/blob/master/2016/Models/2016ComparingSegregatedHighSchoolCampuses.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "results = []\n",
    "\n",
    "def EvaluateClassifierEstimator(classifierEstimator, X, y, cv, model):\n",
    "   \n",
    "    #Perform cross validation \n",
    "    scores = cross_validate(classifierEstimator, inci_X, inci_Y, scoring=['accuracy','precision','recall']\n",
    "                            , cv=cv, return_train_score=True)\n",
    "\n",
    "    Accavg = scores['test_accuracy'].mean()\n",
    "    Preavg = scores['test_precision'].mean()\n",
    "    Recavg = scores['test_recall'].mean()\n",
    "\n",
    "    print_str = \"The average accuracy for all cv folds is: \\t\\t\\t {Accavg:.5}\"\n",
    "    print_str2 = \"The average precision for all cv folds is: \\t\\t\\t {Preavg:.5}\"\n",
    "    print_str3 = \"The average recall for all cv folds is: \\t\\t\\t {Recavg:.5}\"\n",
    "\n",
    "    print(print_str.format(Accavg=Accavg))\n",
    "    print(print_str2.format(Preavg=Preavg))\n",
    "    print(print_str3.format(Recavg=Recavg))\n",
    "    print('*********************************************************')\n",
    "\n",
    "    print('Cross Validation Fold Mean Error Scores')\n",
    "    scoresResults = pd.DataFrame()\n",
    "    scoresResults['Accuracy'] = scores['test_accuracy']\n",
    "    scoresResults['Precision'] = scores['test_precision']\n",
    "    scoresResults['Recall'] = scores['test_recall']\n",
    "    \n",
    "    results.append({'Model': model, 'Accuracy': Accavg, 'Precision': Preavg, 'Recall': Recavg})\n",
    "\n",
    "    return scoresResults\n",
    "\n",
    "def EvaluateClassifierEstimator2(classifierEstimator, X, y, cv):\n",
    "    \n",
    "    #Perform cross validation \n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    predictions = cross_val_predict(classifierEstimator, inci_X, inci_Y, cv=cv)\n",
    "    \n",
    "    #model evaluation \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "    \n",
    "    #pass true test set values and predictions to classification_report\n",
    "    classReport = classification_report(Y,predictions)\n",
    "    confMat = confusion_matrix(Y,predictions)\n",
    "    acc = accuracy_score(Y,predictions)\n",
    "    \n",
    "    print (classReport)\n",
    "    print (confMat)\n",
    "    print (acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV Logistic Regression with Manual Feature Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=8)]: Done 280 out of 280 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=0, test_size=0.2, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=8,\n",
       "       param_grid={'penalty': ['l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'class_weight': ['balanced', 'none'], 'random_state': [0], 'solver': ['lbfgs'], 'max_iter': [100, 500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logisitic regression 10-fold cross-validation \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "regEstimator = LogisticRegression()\n",
    "\n",
    "parameters = { 'penalty':['l2']\n",
    "              ,'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "              ,'class_weight': ['balanced', 'none']\n",
    "              ,'random_state': [0]\n",
    "              ,'solver': ['lbfgs']\n",
    "              ,'max_iter':[100,500]\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "regGridSearch = GridSearchCV(estimator=regEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring='accuracy')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "#regGridSearch.fit(teamX, teamY)\n",
    "regGridSearch.fit(df_inci_X_scaled, inci_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00796553, -0.08838005, -0.17504015,  0.17255479, -0.03045134,\n",
       "        -0.08302298,  0.12905592,  0.02997458, -0.03865177,  0.01099027,\n",
       "         0.05481226, -0.01266453, -0.04454001,  0.00121415,  0.00491569,\n",
       "        -0.05200729, -0.0295495 , -0.09433099,  0.05186489, -0.12319448,\n",
       "         0.12497818, -0.03237182]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Diplay the top model parameters\n",
    "regGridSearch.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuacy Precision, Recall, Attribute Weights, Model Parameters\n",
    "Average accuracy, precision, and recall for the cross-validation folds is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy for all cv folds is: \t\t\t 0.64862\n",
      "The average precision for all cv folds is: \t\t\t 0.51234\n",
      "The average recall for all cv folds is: \t\t\t 0.010762\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n",
      "\n",
      "    Accuracy  Precision  Recall\n",
      "0    0.6480     0.4927  0.0096\n",
      "1    0.6485     0.5048  0.0113\n",
      "2    0.6470     0.4985  0.0119\n",
      "3    0.6460     0.5524  0.0096\n",
      "4    0.6468     0.5074  0.0097\n",
      "5    0.6518     0.5100  0.0109\n",
      "6    0.6516     0.4862  0.0114\n",
      "7    0.6481     0.5226  0.0115\n",
      "8    0.6472     0.5114  0.0111\n",
      "9    0.6511     0.5374  0.0108\n",
      "\n",
      "---- Logistic Regression - CV, Scaled 'Manual' Attr Elimination ----\n",
      "C : \t  0.001\n",
      "class_weight : \t  none\n",
      "max_iter : \t  100\n",
      "penalty : \t  l2\n",
      "random_state : \t  0\n",
      "solver : \t  lbfgs\n",
      "\n",
      "---- Attributes and their weights -----\n",
      "\n",
      "Call_Received_Hour  has weight of 0.17255478852412748\n",
      "Degree_Fact  has weight of 0.1290559193980801\n",
      "UCR__THEFT_FRAUD  has weight of 0.12497818088044535\n",
      "Div_NORTHWEST  has weight of 0.05481226142274191\n",
      "UCR__MVA_TRAFFIC  has weight of 0.0518648947071023\n",
      "Div_CENTRAL  has weight of 0.029974581330874626\n",
      "Div_NORTHEAST  has weight of 0.010990267630886093\n",
      "Div_Unknown  has weight of 0.004915689317016271\n",
      "Div_SOUTHWEST  has weight of 0.0012141509808657799\n",
      "Day1_of_the_Week  has weight of -0.007965527802853352\n",
      "Div_SOUTH CENTRAL  has weight of -0.012664533862031917\n",
      "UCR__INTOXICATION  has weight of -0.029549503868905057\n",
      "IsMale  has weight of -0.030451341708907065\n",
      "UCR__WEAPONS_FIREARMS  has weight of -0.032371824005793956\n",
      "Div_NORTH CENTRAL  has weight of -0.0386517739462726\n",
      "Div_SOUTHEAST  has weight of -0.04454000581539511\n",
      "UCR__ASSUALT  has weight of -0.052007289040154024\n",
      "Social_crime_score  has weight of -0.08302298380521249\n",
      "Number_of_offense  has weight of -0.08838005165996347\n",
      "UCR__MURDER_DEATH  has weight of -0.09433099243552606\n",
      "UCR__OTHERS_THREATS  has weight of -0.123194481708671\n",
      "Arrest_status  has weight of -0.1750401476366185\n"
     ]
    }
   ],
   "source": [
    "#Use the best parameters for our Linear Regression object\n",
    "classifierEst = regGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "print(\"\\n\",round(EvaluateClassifierEstimator(classifierEst, df_inci_X_scaled, inci_Y, cv, \"manual\"),4))\n",
    "\n",
    "#Use the best parameters for our Linear Regression object\",\n",
    "ClassiferParams = regGridSearch.best_params_\n",
    "print(\"\\n---- Logistic Regression - CV, Scaled 'Manual' Attr Elimination ----\")\n",
    "for keys,values in ClassiferParams.items():\n",
    "    print(keys,\": \\t \",values)\n",
    "    \n",
    "# sort these attributes and spit them out\\n\",\n",
    "name = inci_X\n",
    "zip_vars = zip(regGridSearch.best_estimator_.coef_.T,name) # combine attributes\n",
    "zip_vars = sorted(zip_vars, reverse=True)\n",
    "\n",
    "# Print out Attributes and their weights\\n\",\n",
    "print(\"\\n---- Attributes and their weights -----\\n\")\n",
    "for coef, name in zip_vars:\n",
    "    print(name, ' has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Estimator GridSearch Prediction\n",
      "[0 0 0 ... 0 0 0]\n",
      "[[0.73277859 0.26722141]\n",
      " [0.63542657 0.36457343]\n",
      " [0.52470965 0.47529035]\n",
      " ...\n",
      " [0.57282696 0.42717304]\n",
      " [0.62291186 0.37708814]\n",
      " [0.58553968 0.41446032]]\n"
     ]
    }
   ],
   "source": [
    "#Is there a difference between .predict and .best_estimator_.predict?  Nope.\n",
    "print(\"Best Estimator GridSearch Prediction\")\n",
    "print(regGridSearch.best_estimator_.predict(df_inci_X_scaled))\n",
    "print(regGridSearch.best_estimator_.predict_proba(df_inci_X_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearchCV Logistic Regression with Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFECV Logistic Regression 1st Pass\n",
      "Fitting 10 folds for each of 28 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:   44.2s\n",
      "[Parallel(n_jobs=8)]: Done 280 out of 280 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 22 features.\n",
      "Fitting estimator with 21 features.\n",
      "Fitting estimator with 20 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 18 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Ranking [16  3  1  1 12  2  1 13  8 15  6 14  7 18 17  9 11  5  1  4  1 10]\n",
      "Support [False False  True  True False False  True False False False False False\n",
      " False False False False False False  True False  True False]\n",
      "Number of Features: 5\n",
      "Logistic Regression Second Pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=10, random_state=0, test_size=0.2, train_size=None),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'logisticregression__penalty': ['l2'], 'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'logisticregression__class_weight': ['balanced', 'none'], 'logisticregression__random_state': [0], 'logisticregression__solver': ['lbfgs'], 'logisticregression__max_iter': [100, 500]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Credit to:  Jake Drew NC Education Data Set Analysis\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "print(\"RFECV Logistic Regression 1st Pass\")\n",
    "rfecvEstimator = LogisticRegression()\n",
    "\n",
    "parameters = { 'penalty':['l2']\n",
    "              ,'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "              ,'class_weight': ['balanced', 'none']\n",
    "              ,'random_state': [0]\n",
    "              ,'solver': ['lbfgs']\n",
    "              ,'max_iter':[100,500]\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rfecvGridSearch = GridSearchCV(estimator=rfecvEstimator\n",
    "                   , n_jobs=8 # jobs to run in parallel\n",
    "                   , verbose=1 # low verbosity\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring='accuracy')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data using RFECV\n",
    "rfecvGridSearch.fit(df_inci_X_Rfe_scaled, inci_Y)\n",
    "\n",
    "#Use the best parameters for our RFECV Linear Regression object\n",
    "rfecvClassifierEst = rfecvGridSearch.best_estimator_\n",
    "\n",
    "#Recursive Feature Elimination\n",
    "rfecv = RFECV(estimator=rfecvClassifierEst, step=1, cv=cv, scoring='accuracy', verbose=1)\n",
    "#X_BestFeatures = rfecv.fit_transform(teamX, teamY)\n",
    "X_BestFeatures = rfecv.fit_transform(df_inci_X_Rfe_scaled, inci_Y)\n",
    "\n",
    "#Print RFECV Details\n",
    "print(\"Ranking\", rfecv.ranking_)\n",
    "print(\"Support\", rfecv.support_)\n",
    "print(\"Number of Features:\", rfecv.n_features_)\n",
    "\n",
    "print(\"Logistic Regression Second Pass\")\n",
    "#create a pipeline to scale all of the data and perform logistic regression during each grid search step.\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "#Define a range of hyper parameters for grid search\n",
    "parameters = { 'logisticregression__penalty':['l2']\n",
    "              ,'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "              ,'logisticregression__class_weight': ['balanced','none']\n",
    "              ,'logisticregression__random_state': [0]\n",
    "              ,'logisticregression__solver': ['lbfgs']\n",
    "              ,'logisticregression__max_iter':[100,500]\n",
    "             }\n",
    "\n",
    "#Perform the grid search using accuracy as a metric during cross validation.\n",
    "grid = GridSearchCV(pipe, parameters, cv=cv, scoring='accuracy')\n",
    "\n",
    "#Use the best features from recursive feature elimination during the grid search\n",
    "grid.fit(df_inci_X_Rfe_scaled, inci_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average accuracy for all cv folds is: \t\t\t 0.64862\n",
      "The average precision for all cv folds is: \t\t\t 0.51234\n",
      "The average recall for all cv folds is: \t\t\t 0.010762\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.648048</td>\n",
       "      <td>0.492701</td>\n",
       "      <td>0.009555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.648521</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.011263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.647027</td>\n",
       "      <td>0.498525</td>\n",
       "      <td>0.011924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.646030</td>\n",
       "      <td>0.552419</td>\n",
       "      <td>0.009621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.646753</td>\n",
       "      <td>0.507407</td>\n",
       "      <td>0.009655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.651833</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.010939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.651609</td>\n",
       "      <td>0.486239</td>\n",
       "      <td>0.011373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.648073</td>\n",
       "      <td>0.522581</td>\n",
       "      <td>0.011452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.647176</td>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.011076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.651136</td>\n",
       "      <td>0.537367</td>\n",
       "      <td>0.010763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall\n",
       "0  0.648048   0.492701  0.009555\n",
       "1  0.648521   0.504762  0.011263\n",
       "2  0.647027   0.498525  0.011924\n",
       "3  0.646030   0.552419  0.009621\n",
       "4  0.646753   0.507407  0.009655\n",
       "5  0.651833   0.510000  0.010939\n",
       "6  0.651609   0.486239  0.011373\n",
       "7  0.648073   0.522581  0.011452\n",
       "8  0.647176   0.511401  0.011076\n",
       "9  0.651136   0.537367  0.010763"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the best parameters from RFE for our Linear Regression object\n",
    "\n",
    "EvaluateClassifierEstimator(rfecvClassifierEst, df_inci_X_Rfe_scaled, inci_Y, cv, 'Rfe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reiteration of manual feature reduction of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[[0.73277859 0.26722141]\n",
      " [0.63542657 0.36457343]\n",
      " [0.52470965 0.47529035]\n",
      " ...\n",
      " [0.57282696 0.42717304]\n",
      " [0.62291186 0.37708814]\n",
      " [0.58553968 0.41446032]]\n",
      "The average accuracy for all cv folds is: \t\t\t 0.64862\n",
      "The average precision for all cv folds is: \t\t\t 0.51234\n",
      "The average recall for all cv folds is: \t\t\t 0.010762\n",
      "*********************************************************\n",
      "Cross Validation Fold Mean Error Scores\n",
      "\n",
      "    Accuracy  Precision  Recall\n",
      "0    0.6480     0.4927  0.0096\n",
      "1    0.6485     0.5048  0.0113\n",
      "2    0.6470     0.4985  0.0119\n",
      "3    0.6460     0.5524  0.0096\n",
      "4    0.6468     0.5074  0.0097\n",
      "5    0.6518     0.5100  0.0109\n",
      "6    0.6516     0.4862  0.0114\n",
      "7    0.6481     0.5226  0.0115\n",
      "8    0.6472     0.5114  0.0111\n",
      "9    0.6511     0.5374  0.0108\n",
      "\n",
      "---- RFECV Regression - CV, Scaled ----\n",
      "C : \t  0.001\n",
      "class_weight : \t  none\n",
      "max_iter : \t  100\n",
      "penalty : \t  l2\n",
      "random_state : \t  0\n",
      "solver : \t  lbfgs\n",
      "\n",
      "---- Attributes and their weights -----\n",
      "\n",
      "Call_Received_Hour  has weight of 0.17255478852412748\n",
      "Degree_Fact  has weight of 0.1290559193980801\n",
      "UCR__THEFT_FRAUD  has weight of 0.12497818088044535\n",
      "Div_NORTHWEST  has weight of 0.05481226142274191\n",
      "UCR__MVA_TRAFFIC  has weight of 0.0518648947071023\n",
      "Div_CENTRAL  has weight of 0.029974581330874626\n",
      "Div_NORTHEAST  has weight of 0.010990267630886093\n",
      "Div_Unknown  has weight of 0.004915689317016271\n",
      "Div_SOUTHWEST  has weight of 0.0012141509808657799\n",
      "Day1_of_the_Week  has weight of -0.007965527802853352\n",
      "Div_SOUTH CENTRAL  has weight of -0.012664533862031917\n",
      "UCR__INTOXICATION  has weight of -0.029549503868905057\n",
      "IsMale  has weight of -0.030451341708907065\n",
      "UCR__WEAPONS_FIREARMS  has weight of -0.032371824005793956\n",
      "Div_NORTH CENTRAL  has weight of -0.0386517739462726\n",
      "Div_SOUTHEAST  has weight of -0.04454000581539511\n",
      "UCR__ASSUALT  has weight of -0.052007289040154024\n",
      "Social_crime_score  has weight of -0.08302298380521249\n",
      "Number_of_offense  has weight of -0.08838005165996347\n",
      "UCR__MURDER_DEATH  has weight of -0.09433099243552606\n",
      "UCR__OTHERS_THREATS  has weight of -0.123194481708671\n",
      "Arrest_status  has weight of -0.1750401476366185\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_.predict(df_inci_X_Rfe_scaled))\n",
    "print(grid.best_estimator_.predict_proba(df_inci_X_Rfe_scaled))\n",
    "\n",
    "#Use the best parameters for our RFE  Regression object\n",
    "rfecvClassifierEst = rfecvGridSearch.best_estimator_\n",
    "\n",
    "#Evaluate the regression estimator above using our pre-defined cross validation and scoring metrics.\n",
    "print(\"\\n\",round(EvaluateClassifierEstimator(rfecvClassifierEst, df_inci_X_Rfe_scaled, inci_Y, cv, \"manual\"),4))\n",
    "\n",
    "#Use the best parameters for our RFECV Regression object\",\n",
    "rfecvClassiferParams = rfecvGridSearch.best_params_\n",
    "print(\"\\n---- RFECV Regression - CV, Scaled ----\")\n",
    "for keys,values in rfecvClassiferParams.items():\n",
    "    print(keys,\": \\t \",values)\n",
    "    \n",
    "# sort these attributes and spit them out\\n\",\n",
    "name = inci_X\n",
    "zip_vars = zip(rfecvGridSearch.best_estimator_.coef_.T,name) # combine attributes\n",
    "zip_vars = sorted(zip_vars, reverse=True)\n",
    "\n",
    "# Print out Attributes and their weights\\n\",\n",
    "print(\"\\n---- Attributes and their weights -----\\n\")\n",
    "for coef, name in zip_vars:\n",
    "    print(name, ' has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT VECTOR MACHINE (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6478185078195039\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "[[130069      0]\n",
      " [ 70711      0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YejurSKunwar\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#SVM model on main dataframe.\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "#train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(df_inci_X_SVM_scaled, inci_Y)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(df_inci_X_SVM_scaled)\n",
    "\n",
    "acc = mt.accuracy_score(inci_Y,y_hat)\n",
    "conf = mt.confusion_matrix(inci_Y,y_hat)\n",
    "prec = mt.precision_score(inci_Y, y_hat)\n",
    "recall = mt.recall_score(inci_Y, y_hat)\n",
    "print('accuracy:', acc )\n",
    "print('precision:', prec)\n",
    "print('recall:', recall)\n",
    "print(conf)\n",
    "\n",
    "results.append({'Model': 'SVM', 'Accuracy': acc, 'Precision': prec, 'Recall': recall})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142796, 22)\n",
      "(142796,)\n",
      "[72085 70711]\n"
     ]
    }
   ],
   "source": [
    "#look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# SVM based Prediction\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Summary\n",
    "\n",
    "- The three models: \n",
    "    - Logistic regression with manual selection, \n",
    "    - Logistic regression with RFE selection, \n",
    "    - Support Vector Machine (SVM) \n",
    "    \n",
    "These models were executed successfully. The models were cross validated with controls. \n",
    "\n",
    "- Stochastic Gradient Descent was not utilized for the support vector machine model ...................***\n",
    "\n",
    "- The \"GridSearchCV Logistic Regression with manual variable reduction\" model ultimately produced the best accuracy and overall results. The results are summarized in the table below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-401ef0b44253>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Precision'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Recall'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results[['Model', 'Accuracy', 'Precision', 'Recall']]\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Advantages\n",
    "\n",
    "For classification analysis both Logistic regression and Support Vector Machines are common machine learning algorithms for creating models.\n",
    "\n",
    "- Logistic regression\n",
    "\n",
    "    - For maximizing the probability of the data, logistic regression models are generally good. The accuracy of model is acheived at its best in these type of models when data points are distictly separated far away from hyperplane.\n",
    "    - This is more probablistic model.\n",
    "    - RFE (Recursive Feature Elimination) model chooses the peformance of feature and repeats process until all attibutes are analyzed. \n",
    "\n",
    "- Support Vector Machine\n",
    "\n",
    "    - By definition, SVM models tired to score on hyperplane which maximizes the distance closest to margin or support vectors.\n",
    "    - This is deterministic model.\n",
    "    - The SVM model creates hyperplane and puts source data in these dimensional space which is different from original data and is analyzed accordingly. \n",
    "\n",
    "\n",
    "Generally, from the results produced by our models both Logistic Regrerssion and SVM have similar accuracy. However, manual and RFE model of Logistic regression performed well in terms of precision and recall then SVM. In terms of accuracy, manual and RFE model has 89.77%, whereas SVM had 89.74% accuracy, which is with less than 1% of each other. \n",
    "In terms of precision, which means correctly classified classes, RFE and manual models had 60.87% precision. SVM did not perform well for out dataset or our variable selection. Overall, the manual and RFE logistic regression model performed best for our dataset. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Feature Importance for Logistic Regression\n",
    "In logistic models, feature weights will provide us with importance of attribute. We can compare RFE and manual models in terms of weight as both were normalized.\n",
    "\n",
    "###### Manual Variable Selection Model\n",
    "\n",
    "*** EXPLANATION RELATED TO ATTRIBUTES \n",
    "\n",
    "###### Recursive Selection Model\n",
    "\n",
    "*** EXPLANATION RELATED TO ATTRIBUTES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plotCoef(coef, names, t):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names), key=lambda x: abs(x[0])))\n",
    "    plt.figure(figsize=(9,12))\n",
    "    barlist = plt.barh(range(len(names)), imp, align='center')\n",
    "    for x in np.nditer(np.where(np.asarray(list(imp)) < 0)):\n",
    "        barlist[x].set_color('r')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.title(t)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCoef(regGridSearch.best_estimator_.coef_[0], inci_X.columns.values, \"Manual Logistic Features\")\n",
    "list(sorted(zip(regGridSearch.best_estimator_.coef_.ravel(), inci_X.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCoef(grid.best_estimator_.named_steps['logisticregression'].coef_.ravel(), inci_X_Rfe.columns.values, \"Recursive Logistic Features\")\n",
    "list(sorted(zip(grid.best_estimator_.named_steps['logisticregression'].coef_.ravel(), inci_X_Rfe.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting SVM Fields\n",
    "\n",
    "For SVM models, the interpretation of field importance is not as straight forward. Non-linear SVM models create hyperplanes in infinite dimensional space. To accomplish this the source data used in the analysis must be mapped to a higher dimentional space and as a result is very different from the original data. Because of this it is not possible to determine feature weights like we did with the logisitc regessions above.\n",
    "\n",
    "However, we can examine individual features to investigate how SVM approaches classification problems.**** EXPLANATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit To:\n",
    "####http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#R and RA\n",
    "pX = pd.DataFrame()\n",
    "\n",
    "pX['a'] = inci_X_SVM['UCR_Offense_Name']\n",
    "pX['b'] = inci_X_SVM['Number_of_offense']\n",
    "\n",
    "psvc = SVC(kernel='linear', C=0.5, gamma='auto').fit(pX, inci_Y)\n",
    "\n",
    "pXAmin = pX['a'].min() - 1\n",
    "pXAmax = pX['a'].max() + 1\n",
    "pXBmin = pX['b'].min() - 1\n",
    "pXBmax = pX['b'].max() + 1\n",
    "\n",
    "pxx, pyy = np.meshgrid(np.arange(pXAmin, pXAmax, 10), np.arange(pXBmin, pXBmax, 10))\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "pZ = psvc.predict(np.c_[pxx.ravel(), pyy.ravel()])\n",
    "\n",
    "pZ = pZ.reshape(pxx.shape)\n",
    "plt.contourf(pxx, pyy, pZ, cmap=plt.cm.Paired, alpha=0.8)\n",
    "\n",
    "plt.scatter(pX['a'], pX['b'], c=inci_Y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('UCR_Offense_Name')\n",
    "plt.ylabel('Number_of_offense')\n",
    "plt.xlim(pxx.min(), pxx.max())\n",
    "plt.title('SVM:  UCR_Offense_Name and Number_of_offense')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END OF REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svc import SVM\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# X = inci_X_SVM_scaled\n",
    "# y = inci_Y\n",
    "\n",
    "# # Create the validation curve visualizer\n",
    "# cv = StratifiedKFold(12)\n",
    "# param_range = np.logspace(-6, -1, 12)\n",
    "\n",
    "# viz = ValidationCurve(\n",
    "#     SVC(), param_name=\"gamma\", param_range=param_range,\n",
    "#     logx=True, cv=cv, scoring=\"f1_weighted\", n_jobs=8,\n",
    "# )\n",
    "\n",
    "# viz.fit(X, y)\n",
    "# viz.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract the numpy arrays from the data frame\n",
    "X = inci_X.as_matrix()\n",
    "y = inci_Y.as_matrix()\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "logistic = LogisticRegression()\n",
    "visualizer = ROCAUC(logistic)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()             # Draw/show/poof the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
